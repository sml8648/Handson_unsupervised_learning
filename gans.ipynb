{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e46aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Lambda\n",
    "from tensorflow.keras.layers import Embedding, Flatten, dot\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, LeakyReLU, Reshape\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9861974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn    1.0.2\n",
      "tensorflow 2.8.0\n",
      "keras      2.8.0\n",
      "numpy      1.21.6\n"
     ]
    }
   ],
   "source": [
    "import sys, sklearn\n",
    "print(f'sklearn    {sklearn.__version__}')\n",
    "print(f'tensorflow {tf.__version__}')\n",
    "print(f'keras      {keras.__version__}')\n",
    "print(f'numpy      {np.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44da98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9413ac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Default GPU Device : /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 15:49:58.181550: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-31 15:49:58.184214: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-05-31 15:49:58.192603: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-31 15:49:58.192641: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device : {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print('Please install GPU version of TF, if GPU is available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee880e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train = X_train.reshape(48000,-1)\n",
    "X_validation = X_validation.reshape(12000,-1)\n",
    "X_test = X_test.reshape(10000,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81b699c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for TensorFlow\n",
    "X_train_keras = X_train.reshape(48000,28,28,1)\n",
    "X_validation_keras = X_validation.reshape(12000,28,28,1)\n",
    "X_test_keras = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "y_train_keras = to_categorical(y_train)\n",
    "y_validation_keras = to_categorical(y_validation)\n",
    "y_test_keras = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68fc4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation),len(X_train)+ len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7164c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 15:50:29.959024: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-31 15:50:29.959097: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same',\n",
    "                 activation='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb01a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 15:50:39.559427: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-05-31 15:50:39.923264: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9278"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 15:51:00.705006: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 24s 14ms/step - loss: 0.2581 - accuracy: 0.9279 - val_loss: 0.0704 - val_accuracy: 0.9790\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0962 - accuracy: 0.9715 - val_loss: 0.0512 - val_accuracy: 0.9846\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0837 - accuracy: 0.9751 - val_loss: 0.0447 - val_accuracy: 0.9851\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0782 - accuracy: 0.9771 - val_loss: 0.0470 - val_accuracy: 0.9852\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0712 - accuracy: 0.9794 - val_loss: 0.0707 - val_accuracy: 0.9803\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0670 - accuracy: 0.9807 - val_loss: 0.0441 - val_accuracy: 0.9868\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0677 - accuracy: 0.9811 - val_loss: 0.0529 - val_accuracy: 0.9853\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0619 - accuracy: 0.9824 - val_loss: 0.0526 - val_accuracy: 0.9850\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0655 - accuracy: 0.9817 - val_loss: 0.0425 - val_accuracy: 0.9887\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0604 - accuracy: 0.9825 - val_loss: 0.0369 - val_accuracy: 0.9884\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0620 - accuracy: 0.9832 - val_loss: 0.0423 - val_accuracy: 0.9879\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0589 - accuracy: 0.9838 - val_loss: 0.0441 - val_accuracy: 0.9875\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0606 - accuracy: 0.9827 - val_loss: 0.0525 - val_accuracy: 0.9876\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0661 - accuracy: 0.9827 - val_loss: 0.0552 - val_accuracy: 0.9856\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0566 - accuracy: 0.9848 - val_loss: 0.0457 - val_accuracy: 0.9893\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0606 - val_accuracy: 0.9861\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0619 - accuracy: 0.9832 - val_loss: 0.0448 - val_accuracy: 0.9898\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0564 - accuracy: 0.9848 - val_loss: 0.0532 - val_accuracy: 0.9869\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0592 - accuracy: 0.9836 - val_loss: 0.0561 - val_accuracy: 0.9843\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0593 - accuracy: 0.9847 - val_loss: 0.0445 - val_accuracy: 0.9889\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0564 - accuracy: 0.9859 - val_loss: 0.0515 - val_accuracy: 0.9874\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0651 - accuracy: 0.9829 - val_loss: 0.0549 - val_accuracy: 0.9868\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 20s 14ms/step - loss: 0.0539 - accuracy: 0.9863 - val_loss: 0.0456 - val_accuracy: 0.9884\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0652 - accuracy: 0.9842 - val_loss: 0.0607 - val_accuracy: 0.9893\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0596 - accuracy: 0.9849 - val_loss: 0.0487 - val_accuracy: 0.9884\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0633 - accuracy: 0.9844 - val_loss: 0.0394 - val_accuracy: 0.9900\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0521 - accuracy: 0.9862 - val_loss: 0.0435 - val_accuracy: 0.9908\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0642 - accuracy: 0.9839 - val_loss: 0.0532 - val_accuracy: 0.9863\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0602 - accuracy: 0.9850 - val_loss: 0.0526 - val_accuracy: 0.9904\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0593 - accuracy: 0.9850 - val_loss: 0.0602 - val_accuracy: 0.9837\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0684 - accuracy: 0.9837 - val_loss: 0.0544 - val_accuracy: 0.9870\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0628 - accuracy: 0.9843 - val_loss: 0.0424 - val_accuracy: 0.9902\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0539 - accuracy: 0.9866 - val_loss: 0.0512 - val_accuracy: 0.9889\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0611 - accuracy: 0.9850 - val_loss: 0.0540 - val_accuracy: 0.9877\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0526 - accuracy: 0.9856 - val_loss: 0.0544 - val_accuracy: 0.9890\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0649 - accuracy: 0.9845 - val_loss: 0.0434 - val_accuracy: 0.9888\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0623 - accuracy: 0.9843 - val_loss: 0.0582 - val_accuracy: 0.9866\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0572 - accuracy: 0.9859 - val_loss: 0.0568 - val_accuracy: 0.9889\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0627 - accuracy: 0.9850 - val_loss: 0.0505 - val_accuracy: 0.9897\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0559 - accuracy: 0.9864 - val_loss: 0.0480 - val_accuracy: 0.9900\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0531 - accuracy: 0.9865 - val_loss: 0.0531 - val_accuracy: 0.9874\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0664 - accuracy: 0.9842 - val_loss: 0.0612 - val_accuracy: 0.9883\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0620 - accuracy: 0.9854 - val_loss: 0.0504 - val_accuracy: 0.9893\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0578 - accuracy: 0.9860 - val_loss: 0.0589 - val_accuracy: 0.9887\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0652 - accuracy: 0.9850 - val_loss: 0.0495 - val_accuracy: 0.9897\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0557 - accuracy: 0.9869 - val_loss: 0.0582 - val_accuracy: 0.9882\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0582 - accuracy: 0.9861 - val_loss: 0.0575 - val_accuracy: 0.9883\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0716 - accuracy: 0.9829 - val_loss: 0.0611 - val_accuracy: 0.9892\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0574 - accuracy: 0.9860 - val_loss: 0.0546 - val_accuracy: 0.9878\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0610 - accuracy: 0.9854 - val_loss: 0.0508 - val_accuracy: 0.9883\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0547 - accuracy: 0.9866 - val_loss: 0.0513 - val_accuracy: 0.9888\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0531 - accuracy: 0.9868 - val_loss: 0.0441 - val_accuracy: 0.9892\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 23s 16ms/step - loss: 0.0611 - accuracy: 0.9854 - val_loss: 0.0431 - val_accuracy: 0.9889\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0528 - accuracy: 0.9871 - val_loss: 0.0648 - val_accuracy: 0.9865\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0662 - accuracy: 0.9851 - val_loss: 0.0544 - val_accuracy: 0.9887\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0610 - accuracy: 0.9860 - val_loss: 0.0613 - val_accuracy: 0.9882\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 26s 17ms/step - loss: 0.0606 - accuracy: 0.9855 - val_loss: 0.0562 - val_accuracy: 0.9898\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0585 - accuracy: 0.9859 - val_loss: 0.0624 - val_accuracy: 0.9893\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0573 - accuracy: 0.9864 - val_loss: 0.0534 - val_accuracy: 0.9893\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0599 - accuracy: 0.9850 - val_loss: 0.0491 - val_accuracy: 0.9889\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0596 - accuracy: 0.9861 - val_loss: 0.0847 - val_accuracy: 0.9869\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0594 - accuracy: 0.9863 - val_loss: 0.0542 - val_accuracy: 0.9908\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0542 - accuracy: 0.9871 - val_loss: 0.0520 - val_accuracy: 0.9897\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0620 - accuracy: 0.9853 - val_loss: 0.0548 - val_accuracy: 0.9888\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0527 - accuracy: 0.9872 - val_loss: 0.0571 - val_accuracy: 0.9897\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 25s 16ms/step - loss: 0.0558 - accuracy: 0.9872 - val_loss: 0.0705 - val_accuracy: 0.9872\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0634 - accuracy: 0.9855 - val_loss: 0.0620 - val_accuracy: 0.9884\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0555 - accuracy: 0.9870 - val_loss: 0.0540 - val_accuracy: 0.9889\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0698 - accuracy: 0.9849 - val_loss: 0.0538 - val_accuracy: 0.9895\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0511 - accuracy: 0.9881 - val_loss: 0.0536 - val_accuracy: 0.9911\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0634 - accuracy: 0.9860 - val_loss: 0.0488 - val_accuracy: 0.9884\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 23s 16ms/step - loss: 0.0703 - accuracy: 0.9843 - val_loss: 0.0547 - val_accuracy: 0.9896\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0548 - accuracy: 0.9860 - val_loss: 0.0570 - val_accuracy: 0.9899\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0627 - accuracy: 0.9856 - val_loss: 0.0632 - val_accuracy: 0.9877\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 23s 16ms/step - loss: 0.0565 - accuracy: 0.9868 - val_loss: 0.0463 - val_accuracy: 0.9905\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0627 - accuracy: 0.9862 - val_loss: 0.0421 - val_accuracy: 0.9895\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0510 - accuracy: 0.9878 - val_loss: 0.0449 - val_accuracy: 0.9895\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0588 - accuracy: 0.9859 - val_loss: 0.0484 - val_accuracy: 0.9907\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0619 - accuracy: 0.9861 - val_loss: 0.0452 - val_accuracy: 0.9896\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0588 - accuracy: 0.9858 - val_loss: 0.0527 - val_accuracy: 0.9887\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 23s 16ms/step - loss: 0.0579 - accuracy: 0.9863 - val_loss: 0.0580 - val_accuracy: 0.9867\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0549 - accuracy: 0.9870 - val_loss: 0.0719 - val_accuracy: 0.9884\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0634 - accuracy: 0.9865 - val_loss: 0.0481 - val_accuracy: 0.9902\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0620 - accuracy: 0.9871 - val_loss: 0.0588 - val_accuracy: 0.9897\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0588 - accuracy: 0.9874 - val_loss: 0.0583 - val_accuracy: 0.9896\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0626 - accuracy: 0.9861 - val_loss: 0.0486 - val_accuracy: 0.9897\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0609 - accuracy: 0.9861 - val_loss: 0.0448 - val_accuracy: 0.9892\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0615 - accuracy: 0.9864 - val_loss: 0.0651 - val_accuracy: 0.9901\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0617 - accuracy: 0.9860 - val_loss: 0.0469 - val_accuracy: 0.9893\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0548 - accuracy: 0.9877 - val_loss: 0.0460 - val_accuracy: 0.9895\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0645 - accuracy: 0.9864 - val_loss: 0.0711 - val_accuracy: 0.9883\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0687 - accuracy: 0.9850 - val_loss: 0.0639 - val_accuracy: 0.9886\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0601 - accuracy: 0.9865 - val_loss: 0.0791 - val_accuracy: 0.9875\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0617 - accuracy: 0.9864 - val_loss: 0.0655 - val_accuracy: 0.9902\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0611 - accuracy: 0.9866 - val_loss: 0.0517 - val_accuracy: 0.9893\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0670 - accuracy: 0.9856 - val_loss: 0.0641 - val_accuracy: 0.9883\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0622 - accuracy: 0.9860 - val_loss: 0.0605 - val_accuracy: 0.9886\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0590 - accuracy: 0.9871 - val_loss: 0.0493 - val_accuracy: 0.9911\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0613 - accuracy: 0.9861 - val_loss: 0.0556 - val_accuracy: 0.9876\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0590 - accuracy: 0.9871 - val_loss: 0.0607 - val_accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = model.fit(X_train_keras, y_train_keras,\n",
    "                        validation_data=(X_validation_keras, y_validation_keras), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aca11fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.9871041774749756\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuract of CNN\n",
    "print('CNN Final Accuracy', cnn_history.history['accuracy'][-1])\n",
    "pd.Series(cnn_history.history['accuracy']).plot(logy=False)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "510c1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "        \n",
    "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n",
    "                  window=5, input_dim=100, output_depth=1):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "        \n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM\n",
    "        \n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self, x_train):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = x_train\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            \n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], \\\n",
    "                                                      a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, \\\n",
    "                        samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, \\\n",
    "                    noise=None, step=0):\n",
    "        current_path = os.getcwd()\n",
    "        file = os.path.sep.join(['', 'images', 'chapter12', 'synthetic_mnist', ''])\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(current_path+file+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64e546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2113e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 14, 14, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 7, 7, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 4, 4, 256)         819456    \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 4, 4, 512)         3277312   \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 8193      \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 12544)             1266944   \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 12544)            50176     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_16 (Conv2D  (None, 14, 14, 128)      819328    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 14, 14, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 28, 28, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_17 (Conv2D  (None, 28, 28, 64)       204864    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 28, 28, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_18 (Conv2D  (None, 28, 28, 32)       51232     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 28, 28, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_19 (Conv2D  (None, 28, 28, 1)        801       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 19:59:31.070927: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-31 19:59:31.597496: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-31 19:59:33.261938: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 1.186396, acc: 0.507812]  [A loss: 0.201920, acc: 1.000000]\n",
      "1: [D loss: 2.603294, acc: 0.500000]  [A loss: 0.102659, acc: 1.000000]\n",
      "2: [D loss: 1.528350, acc: 0.500000]  [A loss: 0.077793, acc: 1.000000]\n",
      "3: [D loss: 1.178800, acc: 0.500000]  [A loss: 0.057673, acc: 1.000000]\n",
      "4: [D loss: 0.953754, acc: 0.500000]  [A loss: 0.043085, acc: 1.000000]\n",
      "5: [D loss: 0.790659, acc: 0.500000]  [A loss: 0.031167, acc: 1.000000]\n",
      "6: [D loss: 0.677558, acc: 0.500000]  [A loss: 0.022993, acc: 1.000000]\n",
      "7: [D loss: 0.596771, acc: 0.500000]  [A loss: 0.016985, acc: 1.000000]\n",
      "8: [D loss: 0.544484, acc: 0.500000]  [A loss: 0.013805, acc: 1.000000]\n",
      "9: [D loss: 0.509141, acc: 0.500000]  [A loss: 0.011165, acc: 1.000000]\n",
      "10: [D loss: 0.484467, acc: 0.500000]  [A loss: 0.009745, acc: 1.000000]\n",
      "11: [D loss: 0.468446, acc: 0.500000]  [A loss: 0.008680, acc: 1.000000]\n",
      "12: [D loss: 0.461466, acc: 0.500000]  [A loss: 0.008204, acc: 1.000000]\n",
      "13: [D loss: 0.460208, acc: 0.500000]  [A loss: 0.008300, acc: 1.000000]\n",
      "14: [D loss: 0.467615, acc: 0.500000]  [A loss: 0.009032, acc: 1.000000]\n",
      "15: [D loss: 0.480839, acc: 0.500000]  [A loss: 0.010219, acc: 1.000000]\n",
      "16: [D loss: 0.512254, acc: 0.500000]  [A loss: 0.013262, acc: 1.000000]\n",
      "17: [D loss: 0.554944, acc: 0.500000]  [A loss: 0.017491, acc: 1.000000]\n",
      "18: [D loss: 0.636195, acc: 0.500000]  [A loss: 0.027591, acc: 1.000000]\n",
      "19: [D loss: 0.758912, acc: 0.500000]  [A loss: 0.043408, acc: 1.000000]\n",
      "20: [D loss: 0.910932, acc: 0.500000]  [A loss: 0.071806, acc: 1.000000]\n",
      "21: [D loss: 0.972796, acc: 0.500000]  [A loss: 0.088639, acc: 1.000000]\n",
      "22: [D loss: 1.006299, acc: 0.500000]  [A loss: 0.123001, acc: 1.000000]\n",
      "23: [D loss: 1.035881, acc: 0.500000]  [A loss: 0.154005, acc: 1.000000]\n",
      "24: [D loss: 1.076516, acc: 0.500000]  [A loss: 0.200218, acc: 1.000000]\n",
      "25: [D loss: 1.040287, acc: 0.500000]  [A loss: 0.228819, acc: 1.000000]\n",
      "26: [D loss: 0.906989, acc: 0.500000]  [A loss: 0.233398, acc: 1.000000]\n",
      "27: [D loss: 0.876355, acc: 0.500000]  [A loss: 0.250358, acc: 1.000000]\n",
      "28: [D loss: 0.844169, acc: 0.500000]  [A loss: 0.267754, acc: 1.000000]\n",
      "29: [D loss: 0.820498, acc: 0.500000]  [A loss: 0.287209, acc: 1.000000]\n",
      "30: [D loss: 0.793107, acc: 0.500000]  [A loss: 0.305445, acc: 1.000000]\n",
      "31: [D loss: 0.766991, acc: 0.500000]  [A loss: 0.325318, acc: 1.000000]\n",
      "32: [D loss: 0.730589, acc: 0.500000]  [A loss: 0.340190, acc: 1.000000]\n",
      "33: [D loss: 0.715773, acc: 0.500000]  [A loss: 0.358882, acc: 1.000000]\n",
      "34: [D loss: 0.685488, acc: 0.500000]  [A loss: 0.373807, acc: 1.000000]\n",
      "35: [D loss: 0.663387, acc: 0.500000]  [A loss: 0.389310, acc: 1.000000]\n",
      "36: [D loss: 0.636562, acc: 0.500000]  [A loss: 0.402310, acc: 1.000000]\n",
      "37: [D loss: 0.611823, acc: 0.500000]  [A loss: 0.412725, acc: 1.000000]\n",
      "38: [D loss: 0.589631, acc: 0.500000]  [A loss: 0.420995, acc: 1.000000]\n",
      "39: [D loss: 0.574049, acc: 0.500000]  [A loss: 0.429471, acc: 1.000000]\n",
      "40: [D loss: 0.557691, acc: 0.500000]  [A loss: 0.438019, acc: 1.000000]\n",
      "41: [D loss: 0.544392, acc: 0.500000]  [A loss: 0.445931, acc: 1.000000]\n",
      "42: [D loss: 0.529306, acc: 0.500000]  [A loss: 0.453898, acc: 1.000000]\n",
      "43: [D loss: 0.514863, acc: 0.500000]  [A loss: 0.460243, acc: 1.000000]\n",
      "44: [D loss: 0.501848, acc: 0.500000]  [A loss: 0.466843, acc: 1.000000]\n",
      "45: [D loss: 0.490364, acc: 0.500000]  [A loss: 0.472551, acc: 1.000000]\n",
      "46: [D loss: 0.478589, acc: 0.500000]  [A loss: 0.478363, acc: 1.000000]\n",
      "47: [D loss: 0.468494, acc: 0.500000]  [A loss: 0.484329, acc: 1.000000]\n",
      "48: [D loss: 0.457904, acc: 0.500000]  [A loss: 0.489371, acc: 1.000000]\n",
      "49: [D loss: 0.448663, acc: 0.500000]  [A loss: 0.496540, acc: 1.000000]\n",
      "50: [D loss: 0.438428, acc: 0.500000]  [A loss: 0.501094, acc: 1.000000]\n",
      "51: [D loss: 0.428756, acc: 0.500000]  [A loss: 0.508465, acc: 1.000000]\n",
      "52: [D loss: 0.418301, acc: 0.500000]  [A loss: 0.512379, acc: 1.000000]\n",
      "53: [D loss: 0.411060, acc: 0.501953]  [A loss: 0.502490, acc: 1.000000]\n",
      "54: [D loss: 0.420384, acc: 0.500000]  [A loss: 0.513720, acc: 1.000000]\n",
      "55: [D loss: 0.400793, acc: 0.507812]  [A loss: 0.524319, acc: 1.000000]\n",
      "56: [D loss: 0.387982, acc: 0.521484]  [A loss: 0.529083, acc: 1.000000]\n",
      "57: [D loss: 0.374877, acc: 0.564453]  [A loss: 0.535513, acc: 1.000000]\n",
      "58: [D loss: 0.366243, acc: 0.615234]  [A loss: 0.532069, acc: 1.000000]\n",
      "59: [D loss: 0.360691, acc: 0.638672]  [A loss: 0.547936, acc: 1.000000]\n",
      "60: [D loss: 0.345540, acc: 0.769531]  [A loss: 0.542886, acc: 0.996094]\n",
      "61: [D loss: 0.338728, acc: 0.824219]  [A loss: 0.556177, acc: 0.996094]\n",
      "62: [D loss: 0.338933, acc: 0.917969]  [A loss: 0.457980, acc: 1.000000]\n",
      "63: [D loss: 0.423902, acc: 0.515625]  [A loss: 0.533225, acc: 0.996094]\n",
      "64: [D loss: 0.334417, acc: 0.857422]  [A loss: 0.449881, acc: 1.000000]\n",
      "65: [D loss: 0.394489, acc: 0.556641]  [A loss: 0.502869, acc: 1.000000]\n",
      "66: [D loss: 0.331913, acc: 0.847656]  [A loss: 0.494560, acc: 0.996094]\n",
      "67: [D loss: 0.322536, acc: 0.880859]  [A loss: 0.503998, acc: 0.996094]\n",
      "68: [D loss: 0.326326, acc: 0.923828]  [A loss: 0.349732, acc: 1.000000]\n",
      "69: [D loss: 0.504269, acc: 0.501953]  [A loss: 0.494657, acc: 1.000000]\n",
      "70: [D loss: 0.317399, acc: 0.904297]  [A loss: 0.462262, acc: 0.996094]\n",
      "71: [D loss: 0.318022, acc: 0.894531]  [A loss: 0.406600, acc: 1.000000]\n",
      "72: [D loss: 0.380619, acc: 0.728516]  [A loss: 0.340175, acc: 1.000000]\n",
      "73: [D loss: 0.465451, acc: 0.517578]  [A loss: 0.497454, acc: 0.984375]\n",
      "74: [D loss: 0.306399, acc: 0.914062]  [A loss: 0.446903, acc: 1.000000]\n",
      "75: [D loss: 0.302358, acc: 0.914062]  [A loss: 0.452541, acc: 0.992188]\n",
      "76: [D loss: 0.287995, acc: 0.945312]  [A loss: 0.456106, acc: 0.988281]\n",
      "77: [D loss: 0.369474, acc: 0.958984]  [A loss: 0.191141, acc: 1.000000]\n",
      "78: [D loss: 0.564146, acc: 0.503906]  [A loss: 0.451200, acc: 0.992188]\n",
      "79: [D loss: 0.284087, acc: 0.941406]  [A loss: 0.355226, acc: 1.000000]\n",
      "80: [D loss: 0.298177, acc: 0.892578]  [A loss: 0.380163, acc: 0.996094]\n",
      "81: [D loss: 0.332605, acc: 0.902344]  [A loss: 0.237656, acc: 1.000000]\n",
      "82: [D loss: 0.379448, acc: 0.689453]  [A loss: 0.431971, acc: 0.976562]\n",
      "83: [D loss: 0.338798, acc: 0.916016]  [A loss: 0.231074, acc: 1.000000]\n",
      "84: [D loss: 0.352897, acc: 0.740234]  [A loss: 0.420735, acc: 0.976562]\n",
      "85: [D loss: 0.265091, acc: 0.937500]  [A loss: 0.350889, acc: 0.996094]\n",
      "86: [D loss: 0.256941, acc: 0.937500]  [A loss: 0.376700, acc: 0.996094]\n",
      "87: [D loss: 0.276116, acc: 0.953125]  [A loss: 0.082511, acc: 1.000000]\n",
      "88: [D loss: 0.513851, acc: 0.546875]  [A loss: 0.412897, acc: 0.992188]\n",
      "89: [D loss: 0.249954, acc: 0.962891]  [A loss: 0.297216, acc: 0.996094]\n",
      "90: [D loss: 0.249146, acc: 0.951172]  [A loss: 0.319885, acc: 0.996094]\n",
      "91: [D loss: 0.237187, acc: 0.970703]  [A loss: 0.123433, acc: 1.000000]\n",
      "92: [D loss: 0.341301, acc: 0.769531]  [A loss: 0.415066, acc: 0.960938]\n",
      "93: [D loss: 0.245591, acc: 0.943359]  [A loss: 0.294610, acc: 1.000000]\n",
      "94: [D loss: 0.375222, acc: 0.974609]  [A loss: 0.051813, acc: 1.000000]\n",
      "95: [D loss: 0.485464, acc: 0.603516]  [A loss: 0.415860, acc: 0.976562]\n",
      "96: [D loss: 0.275230, acc: 0.892578]  [A loss: 0.281861, acc: 1.000000]\n",
      "97: [D loss: 0.321321, acc: 0.957031]  [A loss: 0.101751, acc: 1.000000]\n",
      "98: [D loss: 0.287323, acc: 0.867188]  [A loss: 0.346499, acc: 0.984375]\n",
      "99: [D loss: 0.274302, acc: 0.886719]  [A loss: 0.307132, acc: 0.996094]\n",
      "100: [D loss: 0.406614, acc: 0.957031]  [A loss: 0.065490, acc: 1.000000]\n",
      "101: [D loss: 0.315705, acc: 0.830078]  [A loss: 0.397836, acc: 0.964844]\n",
      "102: [D loss: 0.284938, acc: 0.875000]  [A loss: 0.308359, acc: 0.996094]\n",
      "103: [D loss: 0.347720, acc: 0.974609]  [A loss: 0.038149, acc: 1.000000]\n",
      "104: [D loss: 0.347661, acc: 0.769531]  [A loss: 0.388567, acc: 0.953125]\n",
      "105: [D loss: 0.256932, acc: 0.914062]  [A loss: 0.272545, acc: 1.000000]\n",
      "106: [D loss: 0.170664, acc: 0.984375]  [A loss: 0.227467, acc: 1.000000]\n",
      "107: [D loss: 0.187045, acc: 0.976562]  [A loss: 0.284505, acc: 0.996094]\n",
      "108: [D loss: 0.193997, acc: 0.972656]  [A loss: 0.300377, acc: 0.992188]\n",
      "109: [D loss: 0.274586, acc: 0.966797]  [A loss: 0.020454, acc: 1.000000]\n",
      "110: [D loss: 0.374172, acc: 0.740234]  [A loss: 0.551582, acc: 0.730469]\n",
      "111: [D loss: 0.175565, acc: 0.978516]  [A loss: 0.173034, acc: 1.000000]\n",
      "112: [D loss: 0.110240, acc: 0.998047]  [A loss: 0.168947, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113: [D loss: 0.129032, acc: 0.996094]  [A loss: 0.040070, acc: 1.000000]\n",
      "114: [D loss: 0.159194, acc: 0.978516]  [A loss: 0.290920, acc: 1.000000]\n",
      "115: [D loss: 0.212441, acc: 0.935547]  [A loss: 0.406917, acc: 0.894531]\n",
      "116: [D loss: 0.186546, acc: 0.964844]  [A loss: 0.297561, acc: 1.000000]\n",
      "117: [D loss: 0.595504, acc: 0.992188]  [A loss: 0.000269, acc: 1.000000]\n",
      "118: [D loss: 1.862353, acc: 0.500000]  [A loss: 0.236181, acc: 1.000000]\n",
      "119: [D loss: 0.233647, acc: 0.943359]  [A loss: 0.139354, acc: 1.000000]\n",
      "120: [D loss: 0.177349, acc: 0.982422]  [A loss: 0.133123, acc: 1.000000]\n",
      "121: [D loss: 0.160581, acc: 0.986328]  [A loss: 0.134355, acc: 1.000000]\n",
      "122: [D loss: 0.151016, acc: 0.986328]  [A loss: 0.137776, acc: 1.000000]\n",
      "123: [D loss: 0.153179, acc: 0.984375]  [A loss: 0.154707, acc: 1.000000]\n",
      "124: [D loss: 0.170170, acc: 0.970703]  [A loss: 0.191767, acc: 1.000000]\n",
      "125: [D loss: 0.224389, acc: 0.927734]  [A loss: 0.311056, acc: 0.972656]\n",
      "126: [D loss: 0.304255, acc: 0.814453]  [A loss: 0.102132, acc: 1.000000]\n",
      "127: [D loss: 0.095756, acc: 1.000000]  [A loss: 0.127741, acc: 1.000000]\n",
      "128: [D loss: 0.120545, acc: 0.996094]  [A loss: 0.197670, acc: 1.000000]\n",
      "129: [D loss: 0.211479, acc: 0.925781]  [A loss: 0.569271, acc: 0.656250]\n",
      "130: [D loss: 0.489267, acc: 0.619141]  [A loss: 1.067933, acc: 0.199219]\n",
      "131: [D loss: 0.059533, acc: 1.000000]  [A loss: 0.043785, acc: 1.000000]\n",
      "132: [D loss: 0.278568, acc: 0.998047]  [A loss: 0.002210, acc: 1.000000]\n",
      "133: [D loss: 0.448144, acc: 0.666016]  [A loss: 0.492925, acc: 0.820312]\n",
      "134: [D loss: 0.093725, acc: 0.998047]  [A loss: 0.083112, acc: 1.000000]\n",
      "135: [D loss: 0.078408, acc: 0.998047]  [A loss: 0.003423, acc: 1.000000]\n",
      "136: [D loss: 0.417238, acc: 0.708984]  [A loss: 0.383161, acc: 0.917969]\n",
      "137: [D loss: 0.109597, acc: 0.994141]  [A loss: 0.089474, acc: 1.000000]\n",
      "138: [D loss: 0.159596, acc: 0.998047]  [A loss: 0.003592, acc: 1.000000]\n",
      "139: [D loss: 0.336065, acc: 0.804688]  [A loss: 0.360213, acc: 0.933594]\n",
      "140: [D loss: 0.117145, acc: 0.996094]  [A loss: 0.076614, acc: 1.000000]\n",
      "141: [D loss: 0.040582, acc: 1.000000]  [A loss: 0.016627, acc: 1.000000]\n",
      "142: [D loss: 0.065194, acc: 1.000000]  [A loss: 0.060018, acc: 1.000000]\n",
      "143: [D loss: 0.040389, acc: 1.000000]  [A loss: 0.052837, acc: 1.000000]\n",
      "144: [D loss: 0.160947, acc: 0.996094]  [A loss: 0.000024, acc: 1.000000]\n",
      "145: [D loss: 2.206311, acc: 0.500000]  [A loss: 0.779037, acc: 0.449219]\n",
      "146: [D loss: 0.183145, acc: 0.968750]  [A loss: 0.042226, acc: 1.000000]\n",
      "147: [D loss: 0.034913, acc: 1.000000]  [A loss: 0.035445, acc: 1.000000]\n",
      "148: [D loss: 0.034472, acc: 1.000000]  [A loss: 0.034141, acc: 1.000000]\n",
      "149: [D loss: 0.079391, acc: 0.998047]  [A loss: 0.004824, acc: 1.000000]\n",
      "150: [D loss: 0.139017, acc: 0.990234]  [A loss: 0.058898, acc: 1.000000]\n",
      "151: [D loss: 0.039962, acc: 1.000000]  [A loss: 0.035807, acc: 1.000000]\n",
      "152: [D loss: 0.038897, acc: 1.000000]  [A loss: 0.035097, acc: 1.000000]\n",
      "153: [D loss: 0.037694, acc: 1.000000]  [A loss: 0.035251, acc: 1.000000]\n",
      "154: [D loss: 0.037874, acc: 1.000000]  [A loss: 0.035599, acc: 1.000000]\n",
      "155: [D loss: 0.037168, acc: 1.000000]  [A loss: 0.035663, acc: 1.000000]\n",
      "156: [D loss: 0.038952, acc: 1.000000]  [A loss: 0.039828, acc: 1.000000]\n",
      "157: [D loss: 0.044409, acc: 1.000000]  [A loss: 0.050733, acc: 1.000000]\n",
      "158: [D loss: 0.067869, acc: 1.000000]  [A loss: 0.144131, acc: 0.996094]\n",
      "159: [D loss: 0.717930, acc: 0.550781]  [A loss: 6.467148, acc: 0.000000]\n",
      "160: [D loss: 0.464031, acc: 0.994141]  [A loss: 0.000774, acc: 1.000000]\n",
      "161: [D loss: 0.488760, acc: 0.595703]  [A loss: 0.016313, acc: 1.000000]\n",
      "162: [D loss: 0.150677, acc: 0.990234]  [A loss: 0.050764, acc: 1.000000]\n",
      "163: [D loss: 0.097481, acc: 1.000000]  [A loss: 0.074212, acc: 1.000000]\n",
      "164: [D loss: 0.091384, acc: 0.998047]  [A loss: 0.013247, acc: 1.000000]\n",
      "165: [D loss: 0.093631, acc: 0.998047]  [A loss: 0.075362, acc: 1.000000]\n",
      "166: [D loss: 0.039537, acc: 1.000000]  [A loss: 0.039022, acc: 1.000000]\n",
      "167: [D loss: 0.029853, acc: 1.000000]  [A loss: 0.034152, acc: 1.000000]\n",
      "168: [D loss: 0.027022, acc: 1.000000]  [A loss: 0.031433, acc: 1.000000]\n",
      "169: [D loss: 0.025868, acc: 1.000000]  [A loss: 0.030038, acc: 1.000000]\n",
      "170: [D loss: 0.022964, acc: 1.000000]  [A loss: 0.027775, acc: 1.000000]\n",
      "171: [D loss: 0.022936, acc: 1.000000]  [A loss: 0.028229, acc: 1.000000]\n",
      "172: [D loss: 0.022570, acc: 1.000000]  [A loss: 0.028832, acc: 1.000000]\n",
      "173: [D loss: 0.023383, acc: 1.000000]  [A loss: 0.030249, acc: 1.000000]\n",
      "174: [D loss: 0.024233, acc: 1.000000]  [A loss: 0.033572, acc: 1.000000]\n",
      "175: [D loss: 0.028352, acc: 1.000000]  [A loss: 0.047906, acc: 1.000000]\n",
      "176: [D loss: 0.057113, acc: 1.000000]  [A loss: 0.419470, acc: 0.820312]\n",
      "177: [D loss: 2.615009, acc: 0.500000]  [A loss: 8.546850, acc: 0.000000]\n",
      "178: [D loss: 1.822909, acc: 0.986328]  [A loss: 0.000182, acc: 1.000000]\n",
      "179: [D loss: 0.449496, acc: 0.660156]  [A loss: 0.000828, acc: 1.000000]\n",
      "180: [D loss: 0.536121, acc: 0.576172]  [A loss: 0.007171, acc: 1.000000]\n",
      "181: [D loss: 0.310898, acc: 0.818359]  [A loss: 0.034615, acc: 1.000000]\n",
      "182: [D loss: 0.231586, acc: 0.929688]  [A loss: 0.093536, acc: 1.000000]\n",
      "183: [D loss: 0.174097, acc: 0.982422]  [A loss: 0.106614, acc: 1.000000]\n",
      "184: [D loss: 0.084591, acc: 1.000000]  [A loss: 0.057172, acc: 1.000000]\n",
      "185: [D loss: 0.058110, acc: 1.000000]  [A loss: 0.044556, acc: 1.000000]\n",
      "186: [D loss: 0.051410, acc: 1.000000]  [A loss: 0.040751, acc: 1.000000]\n",
      "187: [D loss: 0.045122, acc: 1.000000]  [A loss: 0.037328, acc: 1.000000]\n",
      "188: [D loss: 0.041717, acc: 1.000000]  [A loss: 0.034424, acc: 1.000000]\n",
      "189: [D loss: 0.038040, acc: 1.000000]  [A loss: 0.032985, acc: 1.000000]\n",
      "190: [D loss: 0.036023, acc: 1.000000]  [A loss: 0.030987, acc: 1.000000]\n",
      "191: [D loss: 0.033128, acc: 1.000000]  [A loss: 0.028645, acc: 1.000000]\n",
      "192: [D loss: 0.031675, acc: 1.000000]  [A loss: 0.026999, acc: 1.000000]\n",
      "193: [D loss: 0.028909, acc: 1.000000]  [A loss: 0.026499, acc: 1.000000]\n",
      "194: [D loss: 0.028859, acc: 1.000000]  [A loss: 0.025719, acc: 1.000000]\n",
      "195: [D loss: 0.027850, acc: 1.000000]  [A loss: 0.025756, acc: 1.000000]\n",
      "196: [D loss: 0.028948, acc: 1.000000]  [A loss: 0.026780, acc: 1.000000]\n",
      "197: [D loss: 0.027988, acc: 1.000000]  [A loss: 0.026489, acc: 1.000000]\n",
      "198: [D loss: 0.029001, acc: 1.000000]  [A loss: 0.028424, acc: 1.000000]\n",
      "199: [D loss: 0.029815, acc: 1.000000]  [A loss: 0.029858, acc: 1.000000]\n",
      "200: [D loss: 0.031999, acc: 1.000000]  [A loss: 0.035923, acc: 1.000000]\n",
      "201: [D loss: 0.045206, acc: 1.000000]  [A loss: 0.090159, acc: 1.000000]\n",
      "202: [D loss: 0.502671, acc: 0.667969]  [A loss: 9.492944, acc: 0.000000]\n",
      "203: [D loss: 0.012836, acc: 1.000000]  [A loss: 0.241820, acc: 0.972656]\n",
      "204: [D loss: 0.041859, acc: 1.000000]  [A loss: 0.029696, acc: 1.000000]\n",
      "205: [D loss: 0.009404, acc: 1.000000]  [A loss: 0.023187, acc: 1.000000]\n",
      "206: [D loss: 0.008808, acc: 1.000000]  [A loss: 0.020527, acc: 1.000000]\n",
      "207: [D loss: 0.007759, acc: 1.000000]  [A loss: 0.018267, acc: 1.000000]\n",
      "208: [D loss: 0.007513, acc: 1.000000]  [A loss: 0.018158, acc: 1.000000]\n",
      "209: [D loss: 0.007154, acc: 1.000000]  [A loss: 0.015922, acc: 1.000000]\n",
      "210: [D loss: 0.006382, acc: 1.000000]  [A loss: 0.015032, acc: 1.000000]\n",
      "211: [D loss: 0.005840, acc: 1.000000]  [A loss: 0.013978, acc: 1.000000]\n",
      "212: [D loss: 0.005923, acc: 1.000000]  [A loss: 0.013598, acc: 1.000000]\n",
      "213: [D loss: 0.005619, acc: 1.000000]  [A loss: 0.013393, acc: 1.000000]\n",
      "214: [D loss: 0.005219, acc: 1.000000]  [A loss: 0.011759, acc: 1.000000]\n",
      "215: [D loss: 0.005014, acc: 1.000000]  [A loss: 0.011991, acc: 1.000000]\n",
      "216: [D loss: 0.004783, acc: 1.000000]  [A loss: 0.011182, acc: 1.000000]\n",
      "217: [D loss: 0.004684, acc: 1.000000]  [A loss: 0.011153, acc: 1.000000]\n",
      "218: [D loss: 0.004388, acc: 1.000000]  [A loss: 0.010434, acc: 1.000000]\n",
      "219: [D loss: 0.004514, acc: 1.000000]  [A loss: 0.010581, acc: 1.000000]\n",
      "220: [D loss: 0.004348, acc: 1.000000]  [A loss: 0.010375, acc: 1.000000]\n",
      "221: [D loss: 0.004248, acc: 1.000000]  [A loss: 0.010218, acc: 1.000000]\n",
      "222: [D loss: 0.004339, acc: 1.000000]  [A loss: 0.010320, acc: 1.000000]\n",
      "223: [D loss: 0.004174, acc: 1.000000]  [A loss: 0.010525, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224: [D loss: 0.004384, acc: 1.000000]  [A loss: 0.011017, acc: 1.000000]\n",
      "225: [D loss: 0.004344, acc: 1.000000]  [A loss: 0.010621, acc: 1.000000]\n",
      "226: [D loss: 0.004492, acc: 1.000000]  [A loss: 0.012265, acc: 1.000000]\n",
      "227: [D loss: 0.005965, acc: 1.000000]  [A loss: 0.022615, acc: 1.000000]\n",
      "228: [D loss: 0.026014, acc: 1.000000]  [A loss: 4.994256, acc: 0.000000]\n",
      "229: [D loss: 0.659246, acc: 0.632812]  [A loss: 15.392780, acc: 0.000000]\n",
      "230: [D loss: 29.410172, acc: 0.841797]  [A loss: 0.000000, acc: 1.000000]\n",
      "231: [D loss: 4.701394, acc: 0.500000]  [A loss: 0.000002, acc: 1.000000]\n",
      "232: [D loss: 3.691862, acc: 0.500000]  [A loss: 0.000015, acc: 1.000000]\n",
      "233: [D loss: 2.770889, acc: 0.500000]  [A loss: 0.000122, acc: 1.000000]\n",
      "234: [D loss: 1.883245, acc: 0.500000]  [A loss: 0.000830, acc: 1.000000]\n",
      "235: [D loss: 1.076423, acc: 0.501953]  [A loss: 0.004768, acc: 1.000000]\n",
      "236: [D loss: 0.558383, acc: 0.591797]  [A loss: 0.014393, acc: 1.000000]\n",
      "237: [D loss: 0.340964, acc: 0.802734]  [A loss: 0.024950, acc: 1.000000]\n",
      "238: [D loss: 0.242053, acc: 0.900391]  [A loss: 0.029296, acc: 1.000000]\n",
      "239: [D loss: 0.196031, acc: 0.943359]  [A loss: 0.032913, acc: 1.000000]\n",
      "240: [D loss: 0.177795, acc: 0.957031]  [A loss: 0.031660, acc: 1.000000]\n",
      "241: [D loss: 0.164990, acc: 0.960938]  [A loss: 0.032053, acc: 1.000000]\n",
      "242: [D loss: 0.154604, acc: 0.974609]  [A loss: 0.030181, acc: 1.000000]\n",
      "243: [D loss: 0.154699, acc: 0.966797]  [A loss: 0.030797, acc: 1.000000]\n",
      "244: [D loss: 0.148708, acc: 0.974609]  [A loss: 0.030430, acc: 1.000000]\n",
      "245: [D loss: 0.149715, acc: 0.978516]  [A loss: 0.031110, acc: 1.000000]\n",
      "246: [D loss: 0.145716, acc: 0.974609]  [A loss: 0.031342, acc: 1.000000]\n",
      "247: [D loss: 0.147189, acc: 0.988281]  [A loss: 0.030615, acc: 1.000000]\n",
      "248: [D loss: 0.147133, acc: 0.976562]  [A loss: 0.031944, acc: 1.000000]\n",
      "249: [D loss: 0.149448, acc: 0.970703]  [A loss: 0.031870, acc: 1.000000]\n",
      "250: [D loss: 0.150239, acc: 0.974609]  [A loss: 0.031491, acc: 1.000000]\n",
      "251: [D loss: 0.146477, acc: 0.978516]  [A loss: 0.032045, acc: 1.000000]\n",
      "252: [D loss: 0.152882, acc: 0.970703]  [A loss: 0.032661, acc: 1.000000]\n",
      "253: [D loss: 0.157632, acc: 0.974609]  [A loss: 0.034297, acc: 1.000000]\n",
      "254: [D loss: 0.159146, acc: 0.964844]  [A loss: 0.034107, acc: 1.000000]\n",
      "255: [D loss: 0.162004, acc: 0.964844]  [A loss: 0.035426, acc: 1.000000]\n",
      "256: [D loss: 0.173349, acc: 0.945312]  [A loss: 0.037500, acc: 1.000000]\n",
      "257: [D loss: 0.180106, acc: 0.953125]  [A loss: 0.039587, acc: 1.000000]\n",
      "258: [D loss: 0.199154, acc: 0.929688]  [A loss: 0.045578, acc: 1.000000]\n",
      "259: [D loss: 0.230390, acc: 0.900391]  [A loss: 0.050509, acc: 1.000000]\n",
      "260: [D loss: 0.264473, acc: 0.867188]  [A loss: 0.062189, acc: 1.000000]\n",
      "261: [D loss: 0.351099, acc: 0.771484]  [A loss: 0.087133, acc: 1.000000]\n",
      "262: [D loss: 0.563209, acc: 0.625000]  [A loss: 0.144548, acc: 1.000000]\n",
      "263: [D loss: 1.029891, acc: 0.519531]  [A loss: 0.167845, acc: 1.000000]\n",
      "264: [D loss: 0.900505, acc: 0.523438]  [A loss: 0.105187, acc: 1.000000]\n",
      "265: [D loss: 0.297611, acc: 0.822266]  [A loss: 0.036606, acc: 1.000000]\n",
      "266: [D loss: 0.089740, acc: 0.994141]  [A loss: 0.022775, acc: 1.000000]\n",
      "267: [D loss: 0.079334, acc: 0.994141]  [A loss: 0.022573, acc: 1.000000]\n",
      "268: [D loss: 0.075616, acc: 0.998047]  [A loss: 0.021643, acc: 1.000000]\n",
      "269: [D loss: 0.073368, acc: 0.998047]  [A loss: 0.021455, acc: 1.000000]\n",
      "270: [D loss: 0.071667, acc: 1.000000]  [A loss: 0.021524, acc: 1.000000]\n",
      "271: [D loss: 0.075781, acc: 0.996094]  [A loss: 0.023036, acc: 1.000000]\n",
      "272: [D loss: 0.078964, acc: 0.996094]  [A loss: 0.023761, acc: 1.000000]\n",
      "273: [D loss: 0.080722, acc: 0.998047]  [A loss: 0.025254, acc: 1.000000]\n",
      "274: [D loss: 0.094605, acc: 0.996094]  [A loss: 0.031478, acc: 1.000000]\n",
      "275: [D loss: 0.121665, acc: 0.980469]  [A loss: 0.046474, acc: 1.000000]\n",
      "276: [D loss: 0.232615, acc: 0.890625]  [A loss: 0.180350, acc: 1.000000]\n",
      "277: [D loss: 1.960707, acc: 0.500000]  [A loss: 0.992306, acc: 0.304688]\n",
      "278: [D loss: 1.897867, acc: 0.500000]  [A loss: 0.126876, acc: 1.000000]\n",
      "279: [D loss: 0.024960, acc: 1.000000]  [A loss: 0.017415, acc: 1.000000]\n",
      "280: [D loss: 0.024731, acc: 1.000000]  [A loss: 0.017354, acc: 1.000000]\n",
      "281: [D loss: 0.023728, acc: 1.000000]  [A loss: 0.018234, acc: 1.000000]\n",
      "282: [D loss: 0.023822, acc: 1.000000]  [A loss: 0.017806, acc: 1.000000]\n",
      "283: [D loss: 0.023786, acc: 1.000000]  [A loss: 0.017737, acc: 1.000000]\n",
      "284: [D loss: 0.022734, acc: 1.000000]  [A loss: 0.017003, acc: 1.000000]\n",
      "285: [D loss: 0.021113, acc: 1.000000]  [A loss: 0.016132, acc: 1.000000]\n",
      "286: [D loss: 0.021371, acc: 1.000000]  [A loss: 0.015728, acc: 1.000000]\n",
      "287: [D loss: 0.019736, acc: 1.000000]  [A loss: 0.015713, acc: 1.000000]\n",
      "288: [D loss: 0.019275, acc: 1.000000]  [A loss: 0.014318, acc: 1.000000]\n",
      "289: [D loss: 0.018141, acc: 1.000000]  [A loss: 0.014268, acc: 1.000000]\n",
      "290: [D loss: 0.017774, acc: 1.000000]  [A loss: 0.013398, acc: 1.000000]\n",
      "291: [D loss: 0.016640, acc: 1.000000]  [A loss: 0.012888, acc: 1.000000]\n",
      "292: [D loss: 0.016388, acc: 1.000000]  [A loss: 0.012493, acc: 1.000000]\n",
      "293: [D loss: 0.015612, acc: 1.000000]  [A loss: 0.011904, acc: 1.000000]\n",
      "294: [D loss: 0.014516, acc: 1.000000]  [A loss: 0.011247, acc: 1.000000]\n",
      "295: [D loss: 0.014331, acc: 1.000000]  [A loss: 0.011402, acc: 1.000000]\n",
      "296: [D loss: 0.013992, acc: 1.000000]  [A loss: 0.010375, acc: 1.000000]\n",
      "297: [D loss: 0.012764, acc: 1.000000]  [A loss: 0.010297, acc: 1.000000]\n",
      "298: [D loss: 0.011968, acc: 1.000000]  [A loss: 0.009470, acc: 1.000000]\n",
      "299: [D loss: 0.011779, acc: 1.000000]  [A loss: 0.009132, acc: 1.000000]\n",
      "300: [D loss: 0.011700, acc: 1.000000]  [A loss: 0.009581, acc: 1.000000]\n",
      "301: [D loss: 0.011736, acc: 1.000000]  [A loss: 0.008891, acc: 1.000000]\n",
      "302: [D loss: 0.011187, acc: 1.000000]  [A loss: 0.008804, acc: 1.000000]\n",
      "303: [D loss: 0.011224, acc: 1.000000]  [A loss: 0.009087, acc: 1.000000]\n",
      "304: [D loss: 0.011414, acc: 1.000000]  [A loss: 0.009186, acc: 1.000000]\n",
      "305: [D loss: 0.010750, acc: 1.000000]  [A loss: 0.008596, acc: 1.000000]\n",
      "306: [D loss: 0.010205, acc: 1.000000]  [A loss: 0.008024, acc: 1.000000]\n",
      "307: [D loss: 0.009333, acc: 1.000000]  [A loss: 0.007356, acc: 1.000000]\n",
      "308: [D loss: 0.009239, acc: 1.000000]  [A loss: 0.007937, acc: 1.000000]\n",
      "309: [D loss: 0.009710, acc: 1.000000]  [A loss: 0.008721, acc: 1.000000]\n",
      "310: [D loss: 0.011521, acc: 1.000000]  [A loss: 0.012428, acc: 1.000000]\n",
      "311: [D loss: 0.026665, acc: 1.000000]  [A loss: 0.216659, acc: 0.996094]\n",
      "312: [D loss: 3.945957, acc: 0.500000]  [A loss: 8.154210, acc: 0.000000]\n",
      "313: [D loss: 0.001883, acc: 1.000000]  [A loss: 0.372107, acc: 0.871094]\n",
      "314: [D loss: 0.002608, acc: 1.000000]  [A loss: 0.003013, acc: 1.000000]\n",
      "315: [D loss: 0.004763, acc: 1.000000]  [A loss: 0.003475, acc: 1.000000]\n",
      "316: [D loss: 0.005805, acc: 1.000000]  [A loss: 0.004062, acc: 1.000000]\n",
      "317: [D loss: 0.006287, acc: 1.000000]  [A loss: 0.004782, acc: 1.000000]\n",
      "318: [D loss: 0.005972, acc: 1.000000]  [A loss: 0.006082, acc: 1.000000]\n",
      "319: [D loss: 0.005374, acc: 1.000000]  [A loss: 0.006633, acc: 1.000000]\n",
      "320: [D loss: 0.005355, acc: 1.000000]  [A loss: 0.007166, acc: 1.000000]\n",
      "321: [D loss: 0.005282, acc: 1.000000]  [A loss: 0.007635, acc: 1.000000]\n",
      "322: [D loss: 0.004941, acc: 1.000000]  [A loss: 0.008606, acc: 1.000000]\n",
      "323: [D loss: 0.004663, acc: 1.000000]  [A loss: 0.008232, acc: 1.000000]\n",
      "324: [D loss: 0.004182, acc: 1.000000]  [A loss: 0.007824, acc: 1.000000]\n",
      "325: [D loss: 0.004352, acc: 1.000000]  [A loss: 0.008082, acc: 1.000000]\n",
      "326: [D loss: 0.003921, acc: 1.000000]  [A loss: 0.007229, acc: 1.000000]\n",
      "327: [D loss: 0.003671, acc: 1.000000]  [A loss: 0.007237, acc: 1.000000]\n",
      "328: [D loss: 0.003668, acc: 1.000000]  [A loss: 0.006141, acc: 1.000000]\n",
      "329: [D loss: 0.003181, acc: 1.000000]  [A loss: 0.006176, acc: 1.000000]\n",
      "330: [D loss: 0.003132, acc: 1.000000]  [A loss: 0.005605, acc: 1.000000]\n",
      "331: [D loss: 0.002860, acc: 1.000000]  [A loss: 0.005758, acc: 1.000000]\n",
      "332: [D loss: 0.002861, acc: 1.000000]  [A loss: 0.005195, acc: 1.000000]\n",
      "333: [D loss: 0.002723, acc: 1.000000]  [A loss: 0.004802, acc: 1.000000]\n",
      "334: [D loss: 0.002470, acc: 1.000000]  [A loss: 0.004649, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335: [D loss: 0.002368, acc: 1.000000]  [A loss: 0.004574, acc: 1.000000]\n",
      "336: [D loss: 0.002357, acc: 1.000000]  [A loss: 0.004288, acc: 1.000000]\n",
      "337: [D loss: 0.002164, acc: 1.000000]  [A loss: 0.004032, acc: 1.000000]\n",
      "338: [D loss: 0.002166, acc: 1.000000]  [A loss: 0.003479, acc: 1.000000]\n",
      "339: [D loss: 0.001955, acc: 1.000000]  [A loss: 0.003532, acc: 1.000000]\n",
      "340: [D loss: 0.001841, acc: 1.000000]  [A loss: 0.003351, acc: 1.000000]\n",
      "341: [D loss: 0.001714, acc: 1.000000]  [A loss: 0.003106, acc: 1.000000]\n",
      "342: [D loss: 0.001621, acc: 1.000000]  [A loss: 0.002851, acc: 1.000000]\n",
      "343: [D loss: 0.001778, acc: 1.000000]  [A loss: 0.002851, acc: 1.000000]\n",
      "344: [D loss: 0.001531, acc: 1.000000]  [A loss: 0.002700, acc: 1.000000]\n",
      "345: [D loss: 0.001440, acc: 1.000000]  [A loss: 0.002579, acc: 1.000000]\n",
      "346: [D loss: 0.001365, acc: 1.000000]  [A loss: 0.002375, acc: 1.000000]\n",
      "347: [D loss: 0.001353, acc: 1.000000]  [A loss: 0.002207, acc: 1.000000]\n",
      "348: [D loss: 0.001285, acc: 1.000000]  [A loss: 0.002159, acc: 1.000000]\n",
      "349: [D loss: 0.001156, acc: 1.000000]  [A loss: 0.002008, acc: 1.000000]\n",
      "350: [D loss: 0.001018, acc: 1.000000]  [A loss: 0.001796, acc: 1.000000]\n",
      "351: [D loss: 0.001068, acc: 1.000000]  [A loss: 0.001782, acc: 1.000000]\n",
      "352: [D loss: 0.001087, acc: 1.000000]  [A loss: 0.001882, acc: 1.000000]\n",
      "353: [D loss: 0.000968, acc: 1.000000]  [A loss: 0.001561, acc: 1.000000]\n",
      "354: [D loss: 0.000915, acc: 1.000000]  [A loss: 0.001591, acc: 1.000000]\n",
      "355: [D loss: 0.001015, acc: 1.000000]  [A loss: 0.001748, acc: 1.000000]\n",
      "356: [D loss: 0.000921, acc: 1.000000]  [A loss: 0.001630, acc: 1.000000]\n",
      "357: [D loss: 0.001038, acc: 1.000000]  [A loss: 0.001722, acc: 1.000000]\n",
      "358: [D loss: 0.000857, acc: 1.000000]  [A loss: 0.001412, acc: 1.000000]\n",
      "359: [D loss: 0.000817, acc: 1.000000]  [A loss: 0.001323, acc: 1.000000]\n",
      "360: [D loss: 0.000734, acc: 1.000000]  [A loss: 0.001248, acc: 1.000000]\n",
      "361: [D loss: 0.000801, acc: 1.000000]  [A loss: 0.001468, acc: 1.000000]\n",
      "362: [D loss: 0.000845, acc: 1.000000]  [A loss: 0.001385, acc: 1.000000]\n",
      "363: [D loss: 0.000745, acc: 1.000000]  [A loss: 0.001298, acc: 1.000000]\n",
      "364: [D loss: 0.000829, acc: 1.000000]  [A loss: 0.001470, acc: 1.000000]\n",
      "365: [D loss: 0.000820, acc: 1.000000]  [A loss: 0.001405, acc: 1.000000]\n",
      "366: [D loss: 0.000771, acc: 1.000000]  [A loss: 0.001360, acc: 1.000000]\n",
      "367: [D loss: 0.000848, acc: 1.000000]  [A loss: 0.001795, acc: 1.000000]\n",
      "368: [D loss: 0.001444, acc: 1.000000]  [A loss: 0.008880, acc: 1.000000]\n",
      "369: [D loss: 1.640370, acc: 0.521484]  [A loss: 28.281063, acc: 0.000000]\n",
      "370: [D loss: 1.009638, acc: 0.994141]  [A loss: 0.077143, acc: 0.980469]\n",
      "371: [D loss: 0.000874, acc: 1.000000]  [A loss: 0.000688, acc: 1.000000]\n",
      "372: [D loss: 0.001987, acc: 1.000000]  [A loss: 0.000748, acc: 1.000000]\n",
      "373: [D loss: 0.003933, acc: 1.000000]  [A loss: 0.001043, acc: 1.000000]\n",
      "374: [D loss: 0.005841, acc: 1.000000]  [A loss: 0.000864, acc: 1.000000]\n",
      "375: [D loss: 0.006259, acc: 1.000000]  [A loss: 0.001272, acc: 1.000000]\n",
      "376: [D loss: 0.006901, acc: 1.000000]  [A loss: 0.001380, acc: 1.000000]\n",
      "377: [D loss: 0.006199, acc: 1.000000]  [A loss: 0.002088, acc: 1.000000]\n",
      "378: [D loss: 0.006716, acc: 1.000000]  [A loss: 0.002383, acc: 1.000000]\n",
      "379: [D loss: 0.005936, acc: 1.000000]  [A loss: 0.002695, acc: 1.000000]\n",
      "380: [D loss: 0.005112, acc: 1.000000]  [A loss: 0.003652, acc: 1.000000]\n",
      "381: [D loss: 0.005730, acc: 1.000000]  [A loss: 0.003171, acc: 1.000000]\n",
      "382: [D loss: 0.005459, acc: 1.000000]  [A loss: 0.003780, acc: 1.000000]\n",
      "383: [D loss: 0.005556, acc: 1.000000]  [A loss: 0.002722, acc: 1.000000]\n",
      "384: [D loss: 0.004722, acc: 1.000000]  [A loss: 0.002719, acc: 1.000000]\n",
      "385: [D loss: 0.004375, acc: 1.000000]  [A loss: 0.004009, acc: 1.000000]\n",
      "386: [D loss: 0.005406, acc: 1.000000]  [A loss: 0.002781, acc: 1.000000]\n",
      "387: [D loss: 0.004169, acc: 1.000000]  [A loss: 0.002772, acc: 1.000000]\n",
      "388: [D loss: 0.004079, acc: 1.000000]  [A loss: 0.002537, acc: 1.000000]\n",
      "389: [D loss: 0.003697, acc: 1.000000]  [A loss: 0.002401, acc: 1.000000]\n",
      "390: [D loss: 0.003209, acc: 1.000000]  [A loss: 0.002331, acc: 1.000000]\n",
      "391: [D loss: 0.002959, acc: 1.000000]  [A loss: 0.002016, acc: 1.000000]\n",
      "392: [D loss: 0.002953, acc: 1.000000]  [A loss: 0.001989, acc: 1.000000]\n",
      "393: [D loss: 0.002709, acc: 1.000000]  [A loss: 0.001681, acc: 1.000000]\n",
      "394: [D loss: 0.002482, acc: 1.000000]  [A loss: 0.001661, acc: 1.000000]\n",
      "395: [D loss: 0.002236, acc: 1.000000]  [A loss: 0.001917, acc: 1.000000]\n",
      "396: [D loss: 0.002538, acc: 1.000000]  [A loss: 0.001559, acc: 1.000000]\n",
      "397: [D loss: 0.002179, acc: 1.000000]  [A loss: 0.001684, acc: 1.000000]\n",
      "398: [D loss: 0.002326, acc: 1.000000]  [A loss: 0.001485, acc: 1.000000]\n",
      "399: [D loss: 0.002162, acc: 1.000000]  [A loss: 0.001716, acc: 1.000000]\n",
      "400: [D loss: 0.002123, acc: 1.000000]  [A loss: 0.001328, acc: 1.000000]\n",
      "401: [D loss: 0.001856, acc: 1.000000]  [A loss: 0.001253, acc: 1.000000]\n",
      "402: [D loss: 0.001880, acc: 1.000000]  [A loss: 0.001275, acc: 1.000000]\n",
      "403: [D loss: 0.001731, acc: 1.000000]  [A loss: 0.000960, acc: 1.000000]\n",
      "404: [D loss: 0.001476, acc: 1.000000]  [A loss: 0.001132, acc: 1.000000]\n",
      "405: [D loss: 0.001561, acc: 1.000000]  [A loss: 0.000993, acc: 1.000000]\n",
      "406: [D loss: 0.001330, acc: 1.000000]  [A loss: 0.001101, acc: 1.000000]\n",
      "407: [D loss: 0.001360, acc: 1.000000]  [A loss: 0.000803, acc: 1.000000]\n",
      "408: [D loss: 0.001215, acc: 1.000000]  [A loss: 0.000824, acc: 1.000000]\n",
      "409: [D loss: 0.001333, acc: 1.000000]  [A loss: 0.000934, acc: 1.000000]\n",
      "410: [D loss: 0.001305, acc: 1.000000]  [A loss: 0.000849, acc: 1.000000]\n",
      "411: [D loss: 0.001250, acc: 1.000000]  [A loss: 0.001151, acc: 1.000000]\n",
      "412: [D loss: 0.001756, acc: 1.000000]  [A loss: 0.001132, acc: 1.000000]\n",
      "413: [D loss: 0.001353, acc: 1.000000]  [A loss: 0.000755, acc: 1.000000]\n",
      "414: [D loss: 0.001058, acc: 1.000000]  [A loss: 0.000868, acc: 1.000000]\n",
      "415: [D loss: 0.001207, acc: 1.000000]  [A loss: 0.000753, acc: 1.000000]\n",
      "416: [D loss: 0.000995, acc: 1.000000]  [A loss: 0.000712, acc: 1.000000]\n",
      "417: [D loss: 0.001012, acc: 1.000000]  [A loss: 0.000747, acc: 1.000000]\n",
      "418: [D loss: 0.001037, acc: 1.000000]  [A loss: 0.000829, acc: 1.000000]\n",
      "419: [D loss: 0.001269, acc: 1.000000]  [A loss: 0.001133, acc: 1.000000]\n",
      "420: [D loss: 0.001724, acc: 1.000000]  [A loss: 0.001455, acc: 1.000000]\n",
      "421: [D loss: 0.003334, acc: 1.000000]  [A loss: 0.026947, acc: 1.000000]\n",
      "422: [D loss: 9.321350, acc: 0.500000]  [A loss: 32.058762, acc: 0.000000]\n",
      "423: [D loss: 0.000003, acc: 1.000000]  [A loss: 5.538589, acc: 0.011719]\n",
      "424: [D loss: 1.664357, acc: 0.539062]  [A loss: 20.920986, acc: 0.000000]\n",
      "425: [D loss: 0.000000, acc: 1.000000]  [A loss: 1.829440, acc: 0.250000]\n",
      "426: [D loss: 0.064742, acc: 0.982422]  [A loss: 0.000296, acc: 1.000000]\n",
      "427: [D loss: 0.000154, acc: 1.000000]  [A loss: 0.000285, acc: 1.000000]\n",
      "428: [D loss: 0.000235, acc: 1.000000]  [A loss: 0.000320, acc: 1.000000]\n",
      "429: [D loss: 0.000190, acc: 1.000000]  [A loss: 0.000304, acc: 1.000000]\n",
      "430: [D loss: 0.000219, acc: 1.000000]  [A loss: 0.000335, acc: 1.000000]\n",
      "431: [D loss: 0.000202, acc: 1.000000]  [A loss: 0.000337, acc: 1.000000]\n",
      "432: [D loss: 0.000203, acc: 1.000000]  [A loss: 0.000347, acc: 1.000000]\n",
      "433: [D loss: 0.000200, acc: 1.000000]  [A loss: 0.000410, acc: 1.000000]\n",
      "434: [D loss: 0.000211, acc: 1.000000]  [A loss: 0.000398, acc: 1.000000]\n",
      "435: [D loss: 0.000198, acc: 1.000000]  [A loss: 0.000372, acc: 1.000000]\n",
      "436: [D loss: 0.000202, acc: 1.000000]  [A loss: 0.000332, acc: 1.000000]\n",
      "437: [D loss: 0.000278, acc: 1.000000]  [A loss: 0.000328, acc: 1.000000]\n",
      "438: [D loss: 0.000209, acc: 1.000000]  [A loss: 0.000540, acc: 1.000000]\n",
      "439: [D loss: 0.000166, acc: 1.000000]  [A loss: 0.000338, acc: 1.000000]\n",
      "440: [D loss: 0.000132, acc: 1.000000]  [A loss: 0.000330, acc: 1.000000]\n",
      "441: [D loss: 0.000160, acc: 1.000000]  [A loss: 0.000381, acc: 1.000000]\n",
      "442: [D loss: 0.000170, acc: 1.000000]  [A loss: 0.000386, acc: 1.000000]\n",
      "443: [D loss: 0.000214, acc: 1.000000]  [A loss: 0.000369, acc: 1.000000]\n",
      "444: [D loss: 0.000175, acc: 1.000000]  [A loss: 0.000432, acc: 1.000000]\n",
      "445: [D loss: 0.000156, acc: 1.000000]  [A loss: 0.000403, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446: [D loss: 0.000157, acc: 1.000000]  [A loss: 0.000399, acc: 1.000000]\n",
      "447: [D loss: 0.000138, acc: 1.000000]  [A loss: 0.000444, acc: 1.000000]\n",
      "448: [D loss: 0.000121, acc: 1.000000]  [A loss: 0.000471, acc: 1.000000]\n",
      "449: [D loss: 0.000143, acc: 1.000000]  [A loss: 0.000421, acc: 1.000000]\n",
      "450: [D loss: 0.000133, acc: 1.000000]  [A loss: 0.000517, acc: 1.000000]\n",
      "451: [D loss: 0.000186, acc: 1.000000]  [A loss: 0.000361, acc: 1.000000]\n",
      "452: [D loss: 0.000166, acc: 1.000000]  [A loss: 0.000482, acc: 1.000000]\n",
      "453: [D loss: 0.000181, acc: 1.000000]  [A loss: 0.000527, acc: 1.000000]\n",
      "454: [D loss: 0.000131, acc: 1.000000]  [A loss: 0.000437, acc: 1.000000]\n",
      "455: [D loss: 0.000167, acc: 1.000000]  [A loss: 0.000410, acc: 1.000000]\n",
      "456: [D loss: 0.000118, acc: 1.000000]  [A loss: 0.000447, acc: 1.000000]\n",
      "457: [D loss: 0.000122, acc: 1.000000]  [A loss: 0.000429, acc: 1.000000]\n",
      "458: [D loss: 0.000115, acc: 1.000000]  [A loss: 0.000423, acc: 1.000000]\n",
      "459: [D loss: 0.000125, acc: 1.000000]  [A loss: 0.000389, acc: 1.000000]\n",
      "460: [D loss: 0.000129, acc: 1.000000]  [A loss: 0.000466, acc: 1.000000]\n",
      "461: [D loss: 0.000115, acc: 1.000000]  [A loss: 0.000503, acc: 1.000000]\n",
      "462: [D loss: 0.000137, acc: 1.000000]  [A loss: 0.000503, acc: 1.000000]\n",
      "463: [D loss: 0.000111, acc: 1.000000]  [A loss: 0.000503, acc: 1.000000]\n",
      "464: [D loss: 0.000115, acc: 1.000000]  [A loss: 0.000404, acc: 1.000000]\n",
      "465: [D loss: 0.000128, acc: 1.000000]  [A loss: 0.000371, acc: 1.000000]\n",
      "466: [D loss: 0.000111, acc: 1.000000]  [A loss: 0.000473, acc: 1.000000]\n",
      "467: [D loss: 0.000168, acc: 1.000000]  [A loss: 0.000473, acc: 1.000000]\n",
      "468: [D loss: 0.000113, acc: 1.000000]  [A loss: 0.000392, acc: 1.000000]\n",
      "469: [D loss: 0.000138, acc: 1.000000]  [A loss: 0.000435, acc: 1.000000]\n",
      "470: [D loss: 0.000128, acc: 1.000000]  [A loss: 0.000489, acc: 1.000000]\n",
      "471: [D loss: 0.000110, acc: 1.000000]  [A loss: 0.000439, acc: 1.000000]\n",
      "472: [D loss: 0.000115, acc: 1.000000]  [A loss: 0.000472, acc: 1.000000]\n",
      "473: [D loss: 0.000106, acc: 1.000000]  [A loss: 0.000510, acc: 1.000000]\n",
      "474: [D loss: 0.000121, acc: 1.000000]  [A loss: 0.000423, acc: 1.000000]\n",
      "475: [D loss: 0.000119, acc: 1.000000]  [A loss: 0.000404, acc: 1.000000]\n",
      "476: [D loss: 0.000126, acc: 1.000000]  [A loss: 0.000411, acc: 1.000000]\n",
      "477: [D loss: 0.000088, acc: 1.000000]  [A loss: 0.000439, acc: 1.000000]\n",
      "478: [D loss: 0.000098, acc: 1.000000]  [A loss: 0.000371, acc: 1.000000]\n",
      "479: [D loss: 0.000083, acc: 1.000000]  [A loss: 0.000377, acc: 1.000000]\n",
      "480: [D loss: 0.000100, acc: 1.000000]  [A loss: 0.000459, acc: 1.000000]\n",
      "481: [D loss: 0.000131, acc: 1.000000]  [A loss: 0.000324, acc: 1.000000]\n",
      "482: [D loss: 0.000109, acc: 1.000000]  [A loss: 0.000386, acc: 1.000000]\n",
      "483: [D loss: 0.000080, acc: 1.000000]  [A loss: 0.000385, acc: 1.000000]\n",
      "484: [D loss: 0.000077, acc: 1.000000]  [A loss: 0.000359, acc: 1.000000]\n",
      "485: [D loss: 0.000111, acc: 1.000000]  [A loss: 0.000448, acc: 1.000000]\n",
      "486: [D loss: 0.000093, acc: 1.000000]  [A loss: 0.000332, acc: 1.000000]\n",
      "487: [D loss: 0.000104, acc: 1.000000]  [A loss: 0.000360, acc: 1.000000]\n",
      "488: [D loss: 0.000074, acc: 1.000000]  [A loss: 0.000413, acc: 1.000000]\n",
      "489: [D loss: 0.000113, acc: 1.000000]  [A loss: 0.000321, acc: 1.000000]\n",
      "490: [D loss: 0.000098, acc: 1.000000]  [A loss: 0.000331, acc: 1.000000]\n",
      "491: [D loss: 0.000082, acc: 1.000000]  [A loss: 0.000334, acc: 1.000000]\n",
      "492: [D loss: 0.000086, acc: 1.000000]  [A loss: 0.000337, acc: 1.000000]\n",
      "493: [D loss: 0.000102, acc: 1.000000]  [A loss: 0.000290, acc: 1.000000]\n",
      "494: [D loss: 0.000080, acc: 1.000000]  [A loss: 0.000296, acc: 1.000000]\n",
      "495: [D loss: 0.000068, acc: 1.000000]  [A loss: 0.000283, acc: 1.000000]\n",
      "496: [D loss: 0.000089, acc: 1.000000]  [A loss: 0.000385, acc: 1.000000]\n",
      "497: [D loss: 0.000073, acc: 1.000000]  [A loss: 0.000298, acc: 1.000000]\n",
      "498: [D loss: 0.000080, acc: 1.000000]  [A loss: 0.000230, acc: 1.000000]\n",
      "499: [D loss: 0.000088, acc: 1.000000]  [A loss: 0.000313, acc: 1.000000]\n",
      "500: [D loss: 0.000066, acc: 1.000000]  [A loss: 0.000232, acc: 1.000000]\n",
      "501: [D loss: 0.000069, acc: 1.000000]  [A loss: 0.000271, acc: 1.000000]\n",
      "502: [D loss: 0.000090, acc: 1.000000]  [A loss: 0.000262, acc: 1.000000]\n",
      "503: [D loss: 0.000052, acc: 1.000000]  [A loss: 0.000251, acc: 1.000000]\n",
      "504: [D loss: 0.000058, acc: 1.000000]  [A loss: 0.000231, acc: 1.000000]\n",
      "505: [D loss: 0.000066, acc: 1.000000]  [A loss: 0.000236, acc: 1.000000]\n",
      "506: [D loss: 0.000065, acc: 1.000000]  [A loss: 0.000268, acc: 1.000000]\n",
      "507: [D loss: 0.000078, acc: 1.000000]  [A loss: 0.000215, acc: 1.000000]\n",
      "508: [D loss: 0.000056, acc: 1.000000]  [A loss: 0.000270, acc: 1.000000]\n",
      "509: [D loss: 0.000073, acc: 1.000000]  [A loss: 0.000230, acc: 1.000000]\n",
      "510: [D loss: 0.000059, acc: 1.000000]  [A loss: 0.000208, acc: 1.000000]\n",
      "511: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.000221, acc: 1.000000]\n",
      "512: [D loss: 0.000061, acc: 1.000000]  [A loss: 0.000184, acc: 1.000000]\n",
      "513: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.000170, acc: 1.000000]\n",
      "514: [D loss: 0.000042, acc: 1.000000]  [A loss: 0.000181, acc: 1.000000]\n",
      "515: [D loss: 0.000056, acc: 1.000000]  [A loss: 0.000215, acc: 1.000000]\n",
      "516: [D loss: 0.000048, acc: 1.000000]  [A loss: 0.000169, acc: 1.000000]\n",
      "517: [D loss: 0.000046, acc: 1.000000]  [A loss: 0.000150, acc: 1.000000]\n",
      "518: [D loss: 0.000047, acc: 1.000000]  [A loss: 0.000170, acc: 1.000000]\n",
      "519: [D loss: 0.000041, acc: 1.000000]  [A loss: 0.000158, acc: 1.000000]\n",
      "520: [D loss: 0.000042, acc: 1.000000]  [A loss: 0.000164, acc: 1.000000]\n",
      "521: [D loss: 0.000044, acc: 1.000000]  [A loss: 0.000133, acc: 1.000000]\n",
      "522: [D loss: 0.000038, acc: 1.000000]  [A loss: 0.000156, acc: 1.000000]\n",
      "523: [D loss: 0.000043, acc: 1.000000]  [A loss: 0.000165, acc: 1.000000]\n",
      "524: [D loss: 0.000043, acc: 1.000000]  [A loss: 0.000124, acc: 1.000000]\n",
      "525: [D loss: 0.000033, acc: 1.000000]  [A loss: 0.000135, acc: 1.000000]\n",
      "526: [D loss: 0.000042, acc: 1.000000]  [A loss: 0.000175, acc: 1.000000]\n",
      "527: [D loss: 0.000047, acc: 1.000000]  [A loss: 0.000168, acc: 1.000000]\n",
      "528: [D loss: 0.000053, acc: 1.000000]  [A loss: 0.000223, acc: 1.000000]\n",
      "529: [D loss: 0.000065, acc: 1.000000]  [A loss: 0.000316, acc: 1.000000]\n",
      "530: [D loss: 0.000231, acc: 1.000000]  [A loss: 0.883167, acc: 0.519531]\n",
      "531: [D loss: 17.607399, acc: 0.500000]  [A loss: 39.356293, acc: 0.000000]\n",
      "532: [D loss: 0.000000, acc: 1.000000]  [A loss: 4.667134, acc: 0.031250]\n",
      "533: [D loss: 0.069014, acc: 0.972656]  [A loss: 0.000000, acc: 1.000000]\n",
      "534: [D loss: 0.001092, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "535: [D loss: 0.002837, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "536: [D loss: 0.002647, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "537: [D loss: 0.003505, acc: 0.998047]  [A loss: 0.000003, acc: 1.000000]\n",
      "538: [D loss: 0.001392, acc: 1.000000]  [A loss: 0.000003, acc: 1.000000]\n",
      "539: [D loss: 0.001002, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "540: [D loss: 0.000941, acc: 1.000000]  [A loss: 0.000007, acc: 1.000000]\n",
      "541: [D loss: 0.000497, acc: 1.000000]  [A loss: 0.000007, acc: 1.000000]\n",
      "542: [D loss: 0.000535, acc: 1.000000]  [A loss: 0.000007, acc: 1.000000]\n",
      "543: [D loss: 0.000351, acc: 1.000000]  [A loss: 0.000015, acc: 1.000000]\n",
      "544: [D loss: 0.000589, acc: 1.000000]  [A loss: 0.000010, acc: 1.000000]\n",
      "545: [D loss: 0.000283, acc: 1.000000]  [A loss: 0.000018, acc: 1.000000]\n",
      "546: [D loss: 0.000256, acc: 1.000000]  [A loss: 0.000011, acc: 1.000000]\n",
      "547: [D loss: 0.000256, acc: 1.000000]  [A loss: 0.000013, acc: 1.000000]\n",
      "548: [D loss: 0.000270, acc: 1.000000]  [A loss: 0.000016, acc: 1.000000]\n",
      "549: [D loss: 0.000207, acc: 1.000000]  [A loss: 0.000022, acc: 1.000000]\n",
      "550: [D loss: 0.000151, acc: 1.000000]  [A loss: 0.000020, acc: 1.000000]\n",
      "551: [D loss: 0.000202, acc: 1.000000]  [A loss: 0.000025, acc: 1.000000]\n",
      "552: [D loss: 0.000123, acc: 1.000000]  [A loss: 0.000029, acc: 1.000000]\n",
      "553: [D loss: 0.000129, acc: 1.000000]  [A loss: 0.000022, acc: 1.000000]\n",
      "554: [D loss: 0.000110, acc: 1.000000]  [A loss: 0.000025, acc: 1.000000]\n",
      "555: [D loss: 0.000109, acc: 1.000000]  [A loss: 0.000035, acc: 1.000000]\n",
      "556: [D loss: 0.000081, acc: 1.000000]  [A loss: 0.000037, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557: [D loss: 0.000080, acc: 1.000000]  [A loss: 0.000034, acc: 1.000000]\n",
      "558: [D loss: 0.000123, acc: 1.000000]  [A loss: 0.000034, acc: 1.000000]\n",
      "559: [D loss: 0.000066, acc: 1.000000]  [A loss: 0.000041, acc: 1.000000]\n",
      "560: [D loss: 0.000083, acc: 1.000000]  [A loss: 0.000039, acc: 1.000000]\n",
      "561: [D loss: 0.000063, acc: 1.000000]  [A loss: 0.000069, acc: 1.000000]\n",
      "562: [D loss: 0.000073, acc: 1.000000]  [A loss: 0.000041, acc: 1.000000]\n",
      "563: [D loss: 0.000083, acc: 1.000000]  [A loss: 0.000066, acc: 1.000000]\n",
      "564: [D loss: 0.000051, acc: 1.000000]  [A loss: 0.000076, acc: 1.000000]\n",
      "565: [D loss: 0.000042, acc: 1.000000]  [A loss: 0.000066, acc: 1.000000]\n",
      "566: [D loss: 0.000060, acc: 1.000000]  [A loss: 0.000057, acc: 1.000000]\n",
      "567: [D loss: 0.000064, acc: 1.000000]  [A loss: 0.000067, acc: 1.000000]\n",
      "568: [D loss: 0.000036, acc: 1.000000]  [A loss: 0.000084, acc: 1.000000]\n",
      "569: [D loss: 0.000039, acc: 1.000000]  [A loss: 0.000058, acc: 1.000000]\n",
      "570: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.000070, acc: 1.000000]\n",
      "571: [D loss: 0.000055, acc: 1.000000]  [A loss: 0.000080, acc: 1.000000]\n",
      "572: [D loss: 0.000038, acc: 1.000000]  [A loss: 0.000085, acc: 1.000000]\n",
      "573: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000080, acc: 1.000000]\n",
      "574: [D loss: 0.000037, acc: 1.000000]  [A loss: 0.000082, acc: 1.000000]\n",
      "575: [D loss: 0.000039, acc: 1.000000]  [A loss: 0.000121, acc: 1.000000]\n",
      "576: [D loss: 0.000027, acc: 1.000000]  [A loss: 0.000095, acc: 1.000000]\n",
      "577: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000093, acc: 1.000000]\n",
      "578: [D loss: 0.000025, acc: 1.000000]  [A loss: 0.000107, acc: 1.000000]\n",
      "579: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000103, acc: 1.000000]\n",
      "580: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000118, acc: 1.000000]\n",
      "581: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.000102, acc: 1.000000]\n",
      "582: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000108, acc: 1.000000]\n",
      "583: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000084, acc: 1.000000]\n",
      "584: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000293, acc: 1.000000]\n",
      "585: [D loss: 0.000028, acc: 1.000000]  [A loss: 0.000119, acc: 1.000000]\n",
      "586: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000079, acc: 1.000000]\n",
      "587: [D loss: 0.000025, acc: 1.000000]  [A loss: 0.000096, acc: 1.000000]\n",
      "588: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000095, acc: 1.000000]\n",
      "589: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000113, acc: 1.000000]\n",
      "590: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000163, acc: 1.000000]\n",
      "591: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000087, acc: 1.000000]\n",
      "592: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000107, acc: 1.000000]\n",
      "593: [D loss: 0.000025, acc: 1.000000]  [A loss: 0.000113, acc: 1.000000]\n",
      "594: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000112, acc: 1.000000]\n",
      "595: [D loss: 0.000023, acc: 1.000000]  [A loss: 0.000120, acc: 1.000000]\n",
      "596: [D loss: 0.000018, acc: 1.000000]  [A loss: 0.000100, acc: 1.000000]\n",
      "597: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000093, acc: 1.000000]\n",
      "598: [D loss: 0.000018, acc: 1.000000]  [A loss: 0.000123, acc: 1.000000]\n",
      "599: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000125, acc: 1.000000]\n",
      "600: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000105, acc: 1.000000]\n",
      "601: [D loss: 0.000023, acc: 1.000000]  [A loss: 0.000113, acc: 1.000000]\n",
      "602: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000098, acc: 1.000000]\n",
      "603: [D loss: 0.000017, acc: 1.000000]  [A loss: 0.000101, acc: 1.000000]\n",
      "604: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000096, acc: 1.000000]\n",
      "605: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.000104, acc: 1.000000]\n",
      "606: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000136, acc: 1.000000]\n",
      "607: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000110, acc: 1.000000]\n",
      "608: [D loss: 0.000017, acc: 1.000000]  [A loss: 0.000086, acc: 1.000000]\n",
      "609: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000100, acc: 1.000000]\n",
      "610: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000100, acc: 1.000000]\n",
      "611: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000099, acc: 1.000000]\n",
      "612: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000082, acc: 1.000000]\n",
      "613: [D loss: 0.000018, acc: 1.000000]  [A loss: 0.000076, acc: 1.000000]\n",
      "614: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000091, acc: 1.000000]\n",
      "615: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000094, acc: 1.000000]\n",
      "616: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000099, acc: 1.000000]\n",
      "617: [D loss: 0.000012, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "618: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "619: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000099, acc: 1.000000]\n",
      "620: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "621: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000081, acc: 1.000000]\n",
      "622: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "623: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000094, acc: 1.000000]\n",
      "624: [D loss: 0.000017, acc: 1.000000]  [A loss: 0.000071, acc: 1.000000]\n",
      "625: [D loss: 0.000012, acc: 1.000000]  [A loss: 0.000063, acc: 1.000000]\n",
      "626: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000077, acc: 1.000000]\n",
      "627: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000060, acc: 1.000000]\n",
      "628: [D loss: 0.000012, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "629: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000062, acc: 1.000000]\n",
      "630: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000055, acc: 1.000000]\n",
      "631: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000060, acc: 1.000000]\n",
      "632: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000084, acc: 1.000000]\n",
      "633: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000052, acc: 1.000000]\n",
      "634: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000054, acc: 1.000000]\n",
      "635: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000045, acc: 1.000000]\n",
      "636: [D loss: 0.000011, acc: 1.000000]  [A loss: 0.000059, acc: 1.000000]\n",
      "637: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000051, acc: 1.000000]\n",
      "638: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000044, acc: 1.000000]\n",
      "639: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000051, acc: 1.000000]\n",
      "640: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000045, acc: 1.000000]\n",
      "641: [D loss: 0.000011, acc: 1.000000]  [A loss: 0.000051, acc: 1.000000]\n",
      "642: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000043, acc: 1.000000]\n",
      "643: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000040, acc: 1.000000]\n",
      "644: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000043, acc: 1.000000]\n",
      "645: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000041, acc: 1.000000]\n",
      "646: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000041, acc: 1.000000]\n",
      "647: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000063, acc: 1.000000]\n",
      "648: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000063, acc: 1.000000]\n",
      "649: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000059, acc: 1.000000]\n",
      "650: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "651: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000031, acc: 1.000000]\n",
      "652: [D loss: 0.000005, acc: 1.000000]  [A loss: 0.000036, acc: 1.000000]\n",
      "653: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "654: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "655: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000059, acc: 1.000000]\n",
      "656: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000256, acc: 1.000000]\n",
      "657: [D loss: 0.550500, acc: 0.742188]  [A loss: 92.222626, acc: 0.000000]\n",
      "658: [D loss: 164.910324, acc: 0.755859]  [A loss: 0.000000, acc: 1.000000]\n",
      "659: [D loss: 0.017192, acc: 0.988281]  [A loss: 0.000000, acc: 1.000000]\n",
      "660: [D loss: 1.284014, acc: 0.734375]  [A loss: 0.000000, acc: 1.000000]\n",
      "661: [D loss: 4.332644, acc: 0.523438]  [A loss: 0.000000, acc: 1.000000]\n",
      "662: [D loss: 6.276243, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "663: [D loss: 6.768765, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "664: [D loss: 6.362050, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "665: [D loss: 5.241702, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "666: [D loss: 3.780782, acc: 0.503906]  [A loss: 0.000000, acc: 1.000000]\n",
      "667: [D loss: 2.086534, acc: 0.541016]  [A loss: 0.000001, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668: [D loss: 0.777118, acc: 0.681641]  [A loss: 0.000002, acc: 1.000000]\n",
      "669: [D loss: 0.189259, acc: 0.906250]  [A loss: 0.000001, acc: 1.000000]\n",
      "670: [D loss: 0.101083, acc: 0.957031]  [A loss: 0.000002, acc: 1.000000]\n",
      "671: [D loss: 0.068285, acc: 0.984375]  [A loss: 0.000025, acc: 1.000000]\n",
      "672: [D loss: 0.044340, acc: 0.990234]  [A loss: 0.000016, acc: 1.000000]\n",
      "673: [D loss: 0.036708, acc: 0.992188]  [A loss: 0.000016, acc: 1.000000]\n",
      "674: [D loss: 0.022945, acc: 0.998047]  [A loss: 0.000041, acc: 1.000000]\n",
      "675: [D loss: 0.026957, acc: 0.998047]  [A loss: 0.000051, acc: 1.000000]\n",
      "676: [D loss: 0.019823, acc: 0.994141]  [A loss: 0.000018, acc: 1.000000]\n",
      "677: [D loss: 0.014889, acc: 0.998047]  [A loss: 0.000085, acc: 1.000000]\n",
      "678: [D loss: 0.013346, acc: 1.000000]  [A loss: 0.000037, acc: 1.000000]\n",
      "679: [D loss: 0.013016, acc: 0.998047]  [A loss: 0.000041, acc: 1.000000]\n",
      "680: [D loss: 0.012503, acc: 0.996094]  [A loss: 0.000043, acc: 1.000000]\n",
      "681: [D loss: 0.008922, acc: 1.000000]  [A loss: 0.000052, acc: 1.000000]\n",
      "682: [D loss: 0.007104, acc: 1.000000]  [A loss: 0.000093, acc: 1.000000]\n",
      "683: [D loss: 0.007653, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "684: [D loss: 0.005654, acc: 1.000000]  [A loss: 0.000110, acc: 1.000000]\n",
      "685: [D loss: 0.007508, acc: 1.000000]  [A loss: 0.000091, acc: 1.000000]\n",
      "686: [D loss: 0.005720, acc: 1.000000]  [A loss: 0.000052, acc: 1.000000]\n",
      "687: [D loss: 0.006244, acc: 1.000000]  [A loss: 0.000063, acc: 1.000000]\n",
      "688: [D loss: 0.006550, acc: 1.000000]  [A loss: 0.000116, acc: 1.000000]\n",
      "689: [D loss: 0.004698, acc: 1.000000]  [A loss: 0.000052, acc: 1.000000]\n",
      "690: [D loss: 0.004836, acc: 1.000000]  [A loss: 0.000057, acc: 1.000000]\n",
      "691: [D loss: 0.005554, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "692: [D loss: 0.003558, acc: 1.000000]  [A loss: 0.000110, acc: 1.000000]\n",
      "693: [D loss: 0.004465, acc: 1.000000]  [A loss: 0.000194, acc: 1.000000]\n",
      "694: [D loss: 0.005286, acc: 1.000000]  [A loss: 0.000066, acc: 1.000000]\n",
      "695: [D loss: 0.006114, acc: 1.000000]  [A loss: 0.000150, acc: 1.000000]\n",
      "696: [D loss: 0.004963, acc: 1.000000]  [A loss: 0.000194, acc: 1.000000]\n",
      "697: [D loss: 0.007925, acc: 1.000000]  [A loss: 0.000096, acc: 1.000000]\n",
      "698: [D loss: 0.006323, acc: 1.000000]  [A loss: 0.000034, acc: 1.000000]\n",
      "699: [D loss: 0.004652, acc: 1.000000]  [A loss: 0.000041, acc: 1.000000]\n",
      "700: [D loss: 0.003991, acc: 1.000000]  [A loss: 0.000112, acc: 1.000000]\n",
      "701: [D loss: 0.004442, acc: 1.000000]  [A loss: 0.000093, acc: 1.000000]\n",
      "702: [D loss: 0.004560, acc: 1.000000]  [A loss: 0.000060, acc: 1.000000]\n",
      "703: [D loss: 0.004200, acc: 1.000000]  [A loss: 0.000127, acc: 1.000000]\n",
      "704: [D loss: 0.003581, acc: 1.000000]  [A loss: 0.000063, acc: 1.000000]\n",
      "705: [D loss: 0.004400, acc: 1.000000]  [A loss: 0.000121, acc: 1.000000]\n",
      "706: [D loss: 0.003771, acc: 1.000000]  [A loss: 0.000105, acc: 1.000000]\n",
      "707: [D loss: 0.004452, acc: 1.000000]  [A loss: 0.000069, acc: 1.000000]\n",
      "708: [D loss: 0.004552, acc: 1.000000]  [A loss: 0.000079, acc: 1.000000]\n",
      "709: [D loss: 0.003241, acc: 1.000000]  [A loss: 0.000093, acc: 1.000000]\n",
      "710: [D loss: 0.005085, acc: 1.000000]  [A loss: 0.000086, acc: 1.000000]\n",
      "711: [D loss: 0.002823, acc: 1.000000]  [A loss: 0.000079, acc: 1.000000]\n",
      "712: [D loss: 0.003128, acc: 1.000000]  [A loss: 0.000104, acc: 1.000000]\n",
      "713: [D loss: 0.003760, acc: 1.000000]  [A loss: 0.000051, acc: 1.000000]\n",
      "714: [D loss: 0.002507, acc: 1.000000]  [A loss: 0.000048, acc: 1.000000]\n",
      "715: [D loss: 0.002565, acc: 1.000000]  [A loss: 0.000059, acc: 1.000000]\n",
      "716: [D loss: 0.002858, acc: 1.000000]  [A loss: 0.000111, acc: 1.000000]\n",
      "717: [D loss: 0.003334, acc: 1.000000]  [A loss: 0.000074, acc: 1.000000]\n",
      "718: [D loss: 0.003525, acc: 1.000000]  [A loss: 0.000171, acc: 1.000000]\n",
      "719: [D loss: 0.006771, acc: 1.000000]  [A loss: 0.000057, acc: 1.000000]\n",
      "720: [D loss: 0.002798, acc: 1.000000]  [A loss: 0.000129, acc: 1.000000]\n",
      "721: [D loss: 0.003582, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "722: [D loss: 0.002872, acc: 1.000000]  [A loss: 0.000095, acc: 1.000000]\n",
      "723: [D loss: 0.004101, acc: 1.000000]  [A loss: 0.000217, acc: 1.000000]\n",
      "724: [D loss: 0.007918, acc: 1.000000]  [A loss: 0.000115, acc: 1.000000]\n",
      "725: [D loss: 0.002886, acc: 1.000000]  [A loss: 0.000081, acc: 1.000000]\n",
      "726: [D loss: 0.002831, acc: 1.000000]  [A loss: 0.000080, acc: 1.000000]\n",
      "727: [D loss: 0.003057, acc: 1.000000]  [A loss: 0.000066, acc: 1.000000]\n",
      "728: [D loss: 0.002773, acc: 1.000000]  [A loss: 0.000072, acc: 1.000000]\n",
      "729: [D loss: 0.002069, acc: 1.000000]  [A loss: 0.000062, acc: 1.000000]\n",
      "730: [D loss: 0.002124, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "731: [D loss: 0.001442, acc: 1.000000]  [A loss: 0.000060, acc: 1.000000]\n",
      "732: [D loss: 0.002230, acc: 1.000000]  [A loss: 0.000061, acc: 1.000000]\n",
      "733: [D loss: 0.002492, acc: 1.000000]  [A loss: 0.000069, acc: 1.000000]\n",
      "734: [D loss: 0.002451, acc: 1.000000]  [A loss: 0.000080, acc: 1.000000]\n",
      "735: [D loss: 0.002937, acc: 1.000000]  [A loss: 0.000147, acc: 1.000000]\n",
      "736: [D loss: 0.007457, acc: 1.000000]  [A loss: 0.000270, acc: 1.000000]\n",
      "737: [D loss: 0.014179, acc: 1.000000]  [A loss: 0.004102, acc: 1.000000]\n",
      "738: [D loss: 10.732178, acc: 0.500000]  [A loss: 47.556450, acc: 0.000000]\n",
      "739: [D loss: 0.000000, acc: 1.000000]  [A loss: 5.158194, acc: 0.066406]\n",
      "740: [D loss: 9.483026, acc: 0.500000]  [A loss: 25.189949, acc: 0.000000]\n",
      "741: [D loss: 0.000001, acc: 1.000000]  [A loss: 0.098262, acc: 0.964844]\n",
      "742: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.008613, acc: 1.000000]\n",
      "743: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.006148, acc: 1.000000]\n",
      "744: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.002851, acc: 1.000000]\n",
      "745: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.002789, acc: 1.000000]\n",
      "746: [D loss: 0.000023, acc: 1.000000]  [A loss: 0.002731, acc: 1.000000]\n",
      "747: [D loss: 0.000023, acc: 1.000000]  [A loss: 0.002267, acc: 1.000000]\n",
      "748: [D loss: 0.000027, acc: 1.000000]  [A loss: 0.002727, acc: 1.000000]\n",
      "749: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.001682, acc: 1.000000]\n",
      "750: [D loss: 0.000035, acc: 1.000000]  [A loss: 0.001534, acc: 1.000000]\n",
      "751: [D loss: 0.000034, acc: 1.000000]  [A loss: 0.001293, acc: 1.000000]\n",
      "752: [D loss: 0.000036, acc: 1.000000]  [A loss: 0.001476, acc: 1.000000]\n",
      "753: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.001240, acc: 1.000000]\n",
      "754: [D loss: 0.000052, acc: 1.000000]  [A loss: 0.002354, acc: 1.000000]\n",
      "755: [D loss: 0.000064, acc: 1.000000]  [A loss: 0.001355, acc: 1.000000]\n",
      "756: [D loss: 0.000068, acc: 1.000000]  [A loss: 0.000863, acc: 1.000000]\n",
      "757: [D loss: 0.000071, acc: 1.000000]  [A loss: 0.000741, acc: 1.000000]\n",
      "758: [D loss: 0.000074, acc: 1.000000]  [A loss: 0.000970, acc: 1.000000]\n",
      "759: [D loss: 0.000087, acc: 1.000000]  [A loss: 0.000897, acc: 1.000000]\n",
      "760: [D loss: 0.000094, acc: 1.000000]  [A loss: 0.000788, acc: 1.000000]\n",
      "761: [D loss: 0.000106, acc: 1.000000]  [A loss: 0.000676, acc: 1.000000]\n",
      "762: [D loss: 0.000077, acc: 1.000000]  [A loss: 0.000736, acc: 1.000000]\n",
      "763: [D loss: 0.000105, acc: 1.000000]  [A loss: 0.000558, acc: 1.000000]\n",
      "764: [D loss: 0.000075, acc: 1.000000]  [A loss: 0.000729, acc: 1.000000]\n",
      "765: [D loss: 0.000125, acc: 1.000000]  [A loss: 0.000439, acc: 1.000000]\n",
      "766: [D loss: 0.000131, acc: 1.000000]  [A loss: 0.000392, acc: 1.000000]\n",
      "767: [D loss: 0.000088, acc: 1.000000]  [A loss: 0.000456, acc: 1.000000]\n",
      "768: [D loss: 0.000117, acc: 1.000000]  [A loss: 0.000452, acc: 1.000000]\n",
      "769: [D loss: 0.000093, acc: 1.000000]  [A loss: 0.000536, acc: 1.000000]\n",
      "770: [D loss: 0.000130, acc: 1.000000]  [A loss: 0.000405, acc: 1.000000]\n",
      "771: [D loss: 0.000109, acc: 1.000000]  [A loss: 0.000532, acc: 1.000000]\n",
      "772: [D loss: 0.000091, acc: 1.000000]  [A loss: 0.000444, acc: 1.000000]\n",
      "773: [D loss: 0.000145, acc: 1.000000]  [A loss: 0.000585, acc: 1.000000]\n",
      "774: [D loss: 0.000095, acc: 1.000000]  [A loss: 0.000509, acc: 1.000000]\n",
      "775: [D loss: 0.000208, acc: 1.000000]  [A loss: 0.000394, acc: 1.000000]\n",
      "776: [D loss: 0.000108, acc: 1.000000]  [A loss: 0.000412, acc: 1.000000]\n",
      "777: [D loss: 0.000105, acc: 1.000000]  [A loss: 0.000458, acc: 1.000000]\n",
      "778: [D loss: 0.000163, acc: 1.000000]  [A loss: 0.000500, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779: [D loss: 0.000143, acc: 1.000000]  [A loss: 0.000337, acc: 1.000000]\n",
      "780: [D loss: 0.000109, acc: 1.000000]  [A loss: 0.000279, acc: 1.000000]\n",
      "781: [D loss: 0.000146, acc: 1.000000]  [A loss: 0.000375, acc: 1.000000]\n",
      "782: [D loss: 0.000118, acc: 1.000000]  [A loss: 0.000303, acc: 1.000000]\n",
      "783: [D loss: 0.000120, acc: 1.000000]  [A loss: 0.000422, acc: 1.000000]\n",
      "784: [D loss: 0.000128, acc: 1.000000]  [A loss: 0.000454, acc: 1.000000]\n",
      "785: [D loss: 0.000114, acc: 1.000000]  [A loss: 0.000328, acc: 1.000000]\n",
      "786: [D loss: 0.000112, acc: 1.000000]  [A loss: 0.000402, acc: 1.000000]\n",
      "787: [D loss: 0.000123, acc: 1.000000]  [A loss: 0.000289, acc: 1.000000]\n",
      "788: [D loss: 0.000136, acc: 1.000000]  [A loss: 0.000284, acc: 1.000000]\n",
      "789: [D loss: 0.000120, acc: 1.000000]  [A loss: 0.000404, acc: 1.000000]\n",
      "790: [D loss: 0.000141, acc: 1.000000]  [A loss: 0.000439, acc: 1.000000]\n",
      "791: [D loss: 0.000118, acc: 1.000000]  [A loss: 0.000272, acc: 1.000000]\n",
      "792: [D loss: 0.000101, acc: 1.000000]  [A loss: 0.000408, acc: 1.000000]\n",
      "793: [D loss: 0.000117, acc: 1.000000]  [A loss: 0.000266, acc: 1.000000]\n",
      "794: [D loss: 0.000100, acc: 1.000000]  [A loss: 0.000276, acc: 1.000000]\n",
      "795: [D loss: 0.000126, acc: 1.000000]  [A loss: 0.000304, acc: 1.000000]\n",
      "796: [D loss: 0.000095, acc: 1.000000]  [A loss: 0.000347, acc: 1.000000]\n",
      "797: [D loss: 0.000125, acc: 1.000000]  [A loss: 0.000303, acc: 1.000000]\n",
      "798: [D loss: 0.000141, acc: 1.000000]  [A loss: 0.000444, acc: 1.000000]\n",
      "799: [D loss: 0.000116, acc: 1.000000]  [A loss: 0.000411, acc: 1.000000]\n",
      "800: [D loss: 0.000108, acc: 1.000000]  [A loss: 0.000613, acc: 1.000000]\n",
      "801: [D loss: 0.000139, acc: 1.000000]  [A loss: 0.000215, acc: 1.000000]\n",
      "802: [D loss: 0.000113, acc: 1.000000]  [A loss: 0.000199, acc: 1.000000]\n",
      "803: [D loss: 0.000120, acc: 1.000000]  [A loss: 0.000265, acc: 1.000000]\n",
      "804: [D loss: 0.000119, acc: 1.000000]  [A loss: 0.000336, acc: 1.000000]\n",
      "805: [D loss: 0.000113, acc: 1.000000]  [A loss: 0.000313, acc: 1.000000]\n",
      "806: [D loss: 0.000094, acc: 1.000000]  [A loss: 0.000274, acc: 1.000000]\n",
      "807: [D loss: 0.000123, acc: 1.000000]  [A loss: 0.000360, acc: 1.000000]\n",
      "808: [D loss: 0.000115, acc: 1.000000]  [A loss: 0.000325, acc: 1.000000]\n",
      "809: [D loss: 0.000091, acc: 1.000000]  [A loss: 0.000257, acc: 1.000000]\n",
      "810: [D loss: 0.000078, acc: 1.000000]  [A loss: 0.000206, acc: 1.000000]\n",
      "811: [D loss: 0.000099, acc: 1.000000]  [A loss: 0.000281, acc: 1.000000]\n",
      "812: [D loss: 0.000094, acc: 1.000000]  [A loss: 0.000261, acc: 1.000000]\n",
      "813: [D loss: 0.000079, acc: 1.000000]  [A loss: 0.000240, acc: 1.000000]\n",
      "814: [D loss: 0.000083, acc: 1.000000]  [A loss: 0.000163, acc: 1.000000]\n",
      "815: [D loss: 0.000088, acc: 1.000000]  [A loss: 0.000285, acc: 1.000000]\n",
      "816: [D loss: 0.000086, acc: 1.000000]  [A loss: 0.000191, acc: 1.000000]\n",
      "817: [D loss: 0.000074, acc: 1.000000]  [A loss: 0.000264, acc: 1.000000]\n",
      "818: [D loss: 0.000070, acc: 1.000000]  [A loss: 0.000218, acc: 1.000000]\n",
      "819: [D loss: 0.000088, acc: 1.000000]  [A loss: 0.000189, acc: 1.000000]\n",
      "820: [D loss: 0.000065, acc: 1.000000]  [A loss: 0.000322, acc: 1.000000]\n",
      "821: [D loss: 0.000127, acc: 1.000000]  [A loss: 0.000213, acc: 1.000000]\n",
      "822: [D loss: 0.000062, acc: 1.000000]  [A loss: 0.000183, acc: 1.000000]\n",
      "823: [D loss: 0.000064, acc: 1.000000]  [A loss: 0.000158, acc: 1.000000]\n",
      "824: [D loss: 0.000080, acc: 1.000000]  [A loss: 0.000225, acc: 1.000000]\n",
      "825: [D loss: 0.000069, acc: 1.000000]  [A loss: 0.000147, acc: 1.000000]\n",
      "826: [D loss: 0.000077, acc: 1.000000]  [A loss: 0.000382, acc: 1.000000]\n",
      "827: [D loss: 0.000095, acc: 1.000000]  [A loss: 0.000130, acc: 1.000000]\n",
      "828: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.000149, acc: 1.000000]\n",
      "829: [D loss: 0.000051, acc: 1.000000]  [A loss: 0.000155, acc: 1.000000]\n",
      "830: [D loss: 0.000058, acc: 1.000000]  [A loss: 0.000166, acc: 1.000000]\n",
      "831: [D loss: 0.000054, acc: 1.000000]  [A loss: 0.000133, acc: 1.000000]\n",
      "832: [D loss: 0.000060, acc: 1.000000]  [A loss: 0.000170, acc: 1.000000]\n",
      "833: [D loss: 0.000052, acc: 1.000000]  [A loss: 0.000116, acc: 1.000000]\n",
      "834: [D loss: 0.000058, acc: 1.000000]  [A loss: 0.000134, acc: 1.000000]\n",
      "835: [D loss: 0.000039, acc: 1.000000]  [A loss: 0.000119, acc: 1.000000]\n",
      "836: [D loss: 0.000048, acc: 1.000000]  [A loss: 0.000169, acc: 1.000000]\n",
      "837: [D loss: 0.000058, acc: 1.000000]  [A loss: 0.000131, acc: 1.000000]\n",
      "838: [D loss: 0.000047, acc: 1.000000]  [A loss: 0.000162, acc: 1.000000]\n",
      "839: [D loss: 0.000061, acc: 1.000000]  [A loss: 0.000128, acc: 1.000000]\n",
      "840: [D loss: 0.000041, acc: 1.000000]  [A loss: 0.000091, acc: 1.000000]\n",
      "841: [D loss: 0.000034, acc: 1.000000]  [A loss: 0.000121, acc: 1.000000]\n",
      "842: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.000110, acc: 1.000000]\n",
      "843: [D loss: 0.000034, acc: 1.000000]  [A loss: 0.000103, acc: 1.000000]\n",
      "844: [D loss: 0.000040, acc: 1.000000]  [A loss: 0.000120, acc: 1.000000]\n",
      "845: [D loss: 0.000045, acc: 1.000000]  [A loss: 0.000134, acc: 1.000000]\n",
      "846: [D loss: 0.000053, acc: 1.000000]  [A loss: 0.000113, acc: 1.000000]\n",
      "847: [D loss: 0.000035, acc: 1.000000]  [A loss: 0.000094, acc: 1.000000]\n",
      "848: [D loss: 0.000039, acc: 1.000000]  [A loss: 0.000119, acc: 1.000000]\n",
      "849: [D loss: 0.000039, acc: 1.000000]  [A loss: 0.000073, acc: 1.000000]\n",
      "850: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.000074, acc: 1.000000]\n",
      "851: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000091, acc: 1.000000]\n",
      "852: [D loss: 0.000035, acc: 1.000000]  [A loss: 0.000114, acc: 1.000000]\n",
      "853: [D loss: 0.000057, acc: 1.000000]  [A loss: 0.000231, acc: 1.000000]\n",
      "854: [D loss: 0.000308, acc: 1.000000]  [A loss: 4.608782, acc: 0.085938]\n",
      "855: [D loss: 19.090508, acc: 0.500000]  [A loss: 51.876335, acc: 0.000000]\n",
      "856: [D loss: 0.000000, acc: 1.000000]  [A loss: 20.781336, acc: 0.000000]\n",
      "857: [D loss: 0.000000, acc: 1.000000]  [A loss: 0.019111, acc: 1.000000]\n",
      "858: [D loss: 0.000000, acc: 1.000000]  [A loss: 0.006584, acc: 1.000000]\n",
      "859: [D loss: 0.000002, acc: 1.000000]  [A loss: 0.013106, acc: 0.992188]\n",
      "860: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.005435, acc: 1.000000]\n",
      "861: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.003580, acc: 1.000000]\n",
      "862: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.005622, acc: 0.996094]\n",
      "863: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.002327, acc: 1.000000]\n",
      "864: [D loss: 0.000023, acc: 1.000000]  [A loss: 0.002090, acc: 1.000000]\n",
      "865: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.002215, acc: 1.000000]\n",
      "866: [D loss: 0.000028, acc: 1.000000]  [A loss: 0.002062, acc: 1.000000]\n",
      "867: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.001224, acc: 1.000000]\n",
      "868: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.001352, acc: 1.000000]\n",
      "869: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.003341, acc: 1.000000]\n",
      "870: [D loss: 0.000029, acc: 1.000000]  [A loss: 0.001043, acc: 1.000000]\n",
      "871: [D loss: 0.000068, acc: 1.000000]  [A loss: 0.000921, acc: 1.000000]\n",
      "872: [D loss: 0.000033, acc: 1.000000]  [A loss: 0.001053, acc: 1.000000]\n",
      "873: [D loss: 0.000033, acc: 1.000000]  [A loss: 0.001056, acc: 1.000000]\n",
      "874: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.001225, acc: 1.000000]\n",
      "875: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000845, acc: 1.000000]\n",
      "876: [D loss: 0.000027, acc: 1.000000]  [A loss: 0.000598, acc: 1.000000]\n",
      "877: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000574, acc: 1.000000]\n",
      "878: [D loss: 0.000035, acc: 1.000000]  [A loss: 0.000696, acc: 1.000000]\n",
      "879: [D loss: 0.000036, acc: 1.000000]  [A loss: 0.001060, acc: 1.000000]\n",
      "880: [D loss: 0.000030, acc: 1.000000]  [A loss: 0.000679, acc: 1.000000]\n",
      "881: [D loss: 0.000033, acc: 1.000000]  [A loss: 0.000654, acc: 1.000000]\n",
      "882: [D loss: 0.000030, acc: 1.000000]  [A loss: 0.000628, acc: 1.000000]\n",
      "883: [D loss: 0.000040, acc: 1.000000]  [A loss: 0.000415, acc: 1.000000]\n",
      "884: [D loss: 0.000033, acc: 1.000000]  [A loss: 0.000594, acc: 1.000000]\n",
      "885: [D loss: 0.000042, acc: 1.000000]  [A loss: 0.000691, acc: 1.000000]\n",
      "886: [D loss: 0.000036, acc: 1.000000]  [A loss: 0.000533, acc: 1.000000]\n",
      "887: [D loss: 0.000036, acc: 1.000000]  [A loss: 0.000468, acc: 1.000000]\n",
      "888: [D loss: 0.000041, acc: 1.000000]  [A loss: 0.000460, acc: 1.000000]\n",
      "889: [D loss: 0.000041, acc: 1.000000]  [A loss: 0.000528, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890: [D loss: 0.000040, acc: 1.000000]  [A loss: 0.000386, acc: 1.000000]\n",
      "891: [D loss: 0.000048, acc: 1.000000]  [A loss: 0.000528, acc: 1.000000]\n",
      "892: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.000713, acc: 1.000000]\n",
      "893: [D loss: 0.000040, acc: 1.000000]  [A loss: 0.000483, acc: 1.000000]\n",
      "894: [D loss: 0.000048, acc: 1.000000]  [A loss: 0.000581, acc: 1.000000]\n",
      "895: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000559, acc: 1.000000]\n",
      "896: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000418, acc: 1.000000]\n",
      "897: [D loss: 0.000038, acc: 1.000000]  [A loss: 0.000405, acc: 1.000000]\n",
      "898: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.000354, acc: 1.000000]\n",
      "899: [D loss: 0.000049, acc: 1.000000]  [A loss: 0.000360, acc: 1.000000]\n",
      "900: [D loss: 0.000035, acc: 1.000000]  [A loss: 0.000477, acc: 1.000000]\n",
      "901: [D loss: 0.000030, acc: 1.000000]  [A loss: 0.000361, acc: 1.000000]\n",
      "902: [D loss: 0.000056, acc: 1.000000]  [A loss: 0.000376, acc: 1.000000]\n",
      "903: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.000328, acc: 1.000000]\n",
      "904: [D loss: 0.000030, acc: 1.000000]  [A loss: 0.000286, acc: 1.000000]\n",
      "905: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000312, acc: 1.000000]\n",
      "906: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.000398, acc: 1.000000]\n",
      "907: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000266, acc: 1.000000]\n",
      "908: [D loss: 0.000035, acc: 1.000000]  [A loss: 0.000368, acc: 1.000000]\n",
      "909: [D loss: 0.000030, acc: 1.000000]  [A loss: 0.000326, acc: 1.000000]\n",
      "910: [D loss: 0.000030, acc: 1.000000]  [A loss: 0.000407, acc: 1.000000]\n",
      "911: [D loss: 0.000033, acc: 1.000000]  [A loss: 0.000240, acc: 1.000000]\n",
      "912: [D loss: 0.000031, acc: 1.000000]  [A loss: 0.000313, acc: 1.000000]\n",
      "913: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.000332, acc: 1.000000]\n",
      "914: [D loss: 0.000027, acc: 1.000000]  [A loss: 0.000261, acc: 1.000000]\n",
      "915: [D loss: 0.000029, acc: 1.000000]  [A loss: 0.000227, acc: 1.000000]\n",
      "916: [D loss: 0.000029, acc: 1.000000]  [A loss: 0.000243, acc: 1.000000]\n",
      "917: [D loss: 0.000032, acc: 1.000000]  [A loss: 0.000250, acc: 1.000000]\n",
      "918: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000294, acc: 1.000000]\n",
      "919: [D loss: 0.000034, acc: 1.000000]  [A loss: 0.000230, acc: 1.000000]\n",
      "920: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000207, acc: 1.000000]\n",
      "921: [D loss: 0.000025, acc: 1.000000]  [A loss: 0.000313, acc: 1.000000]\n",
      "922: [D loss: 0.000029, acc: 1.000000]  [A loss: 0.000230, acc: 1.000000]\n",
      "923: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000238, acc: 1.000000]\n",
      "924: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000179, acc: 1.000000]\n",
      "925: [D loss: 0.000024, acc: 1.000000]  [A loss: 0.000185, acc: 1.000000]\n",
      "926: [D loss: 0.000022, acc: 1.000000]  [A loss: 0.000254, acc: 1.000000]\n",
      "927: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000197, acc: 1.000000]\n",
      "928: [D loss: 0.000022, acc: 1.000000]  [A loss: 0.000161, acc: 1.000000]\n",
      "929: [D loss: 0.000023, acc: 1.000000]  [A loss: 0.000194, acc: 1.000000]\n",
      "930: [D loss: 0.000018, acc: 1.000000]  [A loss: 0.000161, acc: 1.000000]\n",
      "931: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000170, acc: 1.000000]\n",
      "932: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000194, acc: 1.000000]\n",
      "933: [D loss: 0.000017, acc: 1.000000]  [A loss: 0.000207, acc: 1.000000]\n",
      "934: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000145, acc: 1.000000]\n",
      "935: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000155, acc: 1.000000]\n",
      "936: [D loss: 0.000016, acc: 1.000000]  [A loss: 0.000151, acc: 1.000000]\n",
      "937: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000147, acc: 1.000000]\n",
      "938: [D loss: 0.000019, acc: 1.000000]  [A loss: 0.000145, acc: 1.000000]\n",
      "939: [D loss: 0.000017, acc: 1.000000]  [A loss: 0.000147, acc: 1.000000]\n",
      "940: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000146, acc: 1.000000]\n",
      "941: [D loss: 0.000020, acc: 1.000000]  [A loss: 0.000162, acc: 1.000000]\n",
      "942: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000125, acc: 1.000000]\n",
      "943: [D loss: 0.000021, acc: 1.000000]  [A loss: 0.000109, acc: 1.000000]\n",
      "944: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000137, acc: 1.000000]\n",
      "945: [D loss: 0.000014, acc: 1.000000]  [A loss: 0.000123, acc: 1.000000]\n",
      "946: [D loss: 0.000012, acc: 1.000000]  [A loss: 0.000127, acc: 1.000000]\n",
      "947: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000083, acc: 1.000000]\n",
      "948: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000099, acc: 1.000000]\n",
      "949: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000086, acc: 1.000000]\n",
      "950: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000080, acc: 1.000000]\n",
      "951: [D loss: 0.000012, acc: 1.000000]  [A loss: 0.000075, acc: 1.000000]\n",
      "952: [D loss: 0.000011, acc: 1.000000]  [A loss: 0.000121, acc: 1.000000]\n",
      "953: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000105, acc: 1.000000]\n",
      "954: [D loss: 0.000013, acc: 1.000000]  [A loss: 0.000066, acc: 1.000000]\n",
      "955: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000068, acc: 1.000000]\n",
      "956: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000078, acc: 1.000000]\n",
      "957: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "958: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000061, acc: 1.000000]\n",
      "959: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000061, acc: 1.000000]\n",
      "960: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000085, acc: 1.000000]\n",
      "961: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000063, acc: 1.000000]\n",
      "962: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000064, acc: 1.000000]\n",
      "963: [D loss: 0.000009, acc: 1.000000]  [A loss: 0.000051, acc: 1.000000]\n",
      "964: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000068, acc: 1.000000]\n",
      "965: [D loss: 0.000011, acc: 1.000000]  [A loss: 0.000070, acc: 1.000000]\n",
      "966: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "967: [D loss: 0.000006, acc: 1.000000]  [A loss: 0.000056, acc: 1.000000]\n",
      "968: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000070, acc: 1.000000]\n",
      "969: [D loss: 0.000010, acc: 1.000000]  [A loss: 0.000048, acc: 1.000000]\n",
      "970: [D loss: 0.000005, acc: 1.000000]  [A loss: 0.000054, acc: 1.000000]\n",
      "971: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "972: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000052, acc: 1.000000]\n",
      "973: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000053, acc: 1.000000]\n",
      "974: [D loss: 0.000008, acc: 1.000000]  [A loss: 0.000045, acc: 1.000000]\n",
      "975: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000049, acc: 1.000000]\n",
      "976: [D loss: 0.000007, acc: 1.000000]  [A loss: 0.000039, acc: 1.000000]\n",
      "977: [D loss: 0.000005, acc: 1.000000]  [A loss: 0.000062, acc: 1.000000]\n",
      "978: [D loss: 0.000015, acc: 1.000000]  [A loss: 0.000191, acc: 1.000000]\n",
      "979: [D loss: 0.000742, acc: 1.000000]  [A loss: 82.167786, acc: 0.000000]\n",
      "980: [D loss: 0.000000, acc: 1.000000]  [A loss: 19.146912, acc: 0.000000]\n",
      "981: [D loss: 2.567590, acc: 0.537109]  [A loss: 87.554382, acc: 0.000000]\n",
      "982: [D loss: 17.145931, acc: 0.974609]  [A loss: 0.000000, acc: 1.000000]\n",
      "983: [D loss: 0.000000, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "984: [D loss: 0.000000, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "985: [D loss: 0.000026, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "986: [D loss: 0.020543, acc: 0.988281]  [A loss: 0.000000, acc: 1.000000]\n",
      "987: [D loss: 0.210082, acc: 0.929688]  [A loss: 0.000004, acc: 1.000000]\n",
      "988: [D loss: 0.137153, acc: 0.935547]  [A loss: 0.000082, acc: 1.000000]\n",
      "989: [D loss: 0.078932, acc: 0.968750]  [A loss: 0.009903, acc: 0.992188]\n",
      "990: [D loss: 0.432179, acc: 0.814453]  [A loss: 0.244976, acc: 0.914062]\n",
      "991: [D loss: 0.997271, acc: 0.638672]  [A loss: 12.759013, acc: 0.000000]\n",
      "992: [D loss: 12.991112, acc: 0.500000]  [A loss: 11.586763, acc: 0.000000]\n",
      "993: [D loss: 3.438986, acc: 0.511719]  [A loss: 13.381546, acc: 0.000000]\n",
      "994: [D loss: 0.010935, acc: 1.000000]  [A loss: 0.000004, acc: 1.000000]\n",
      "995: [D loss: 0.004378, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "996: [D loss: 0.003289, acc: 1.000000]  [A loss: 0.000002, acc: 1.000000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m mnist_dcgan \u001b[38;5;241m=\u001b[39m MNIST_DCGAN(X_train_keras)\n\u001b[1;32m      3\u001b[0m timer \u001b[38;5;241m=\u001b[39m ElapsedTimer()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmnist_dcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mMNIST_DCGAN.train\u001b[0;34m(self, train_steps, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    127\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones([\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_size, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    128\u001b[0m y[batch_size:, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 130\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones([batch_size, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    133\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, size\u001b[38;5;241m=\u001b[39m[batch_size, \u001b[38;5;241m100\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/keras/engine/training.py:2095\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2092\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m   2093\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m-> 2095\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[1;32m   2097\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m logs\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/keras/utils/tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/keras/utils/tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    555\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 557\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniforge3/envs/krc/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize MNIST DCGAN and train\n",
    "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
    "timer = ElapsedTimer()\n",
    "mnist_dcgan.train(train_steps=, batch_size=256, save_interval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41afb268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 34.916632449626924 min \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 20:34:26.143946: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "timer.elapsed_time()\n",
    "mnist_dcgan.plot_images(fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a988d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
