{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dimensionality_reduction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM6IAFctiTyrrJUT4hMaQMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sml8648/Handson_unsupervised_learning/blob/main/dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9IQyGDN6yo"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "'''Main'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, pickle, gzip\n",
        "import datetime\n",
        "\n",
        "'''Data Prep'''\n",
        "from sklearn import preprocessing as pp \n",
        "\n",
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "pIQjtnZEOXBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "ALFXua0VOAWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "metadata": {
        "id": "cfM0gfePO82g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(48000,-1)\n",
        "X_validation = X_validation.reshape(12000,-1)\n",
        "X_test = X_test.reshape(10000,-1)"
      ],
      "metadata": {
        "id": "e1-GOxIAP7Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify shape of datasets\n",
        "print(\"Shape of X_train: \", X_train.shape)\n",
        "print(\"Shape of y_train: \", y_train.shape)\n",
        "print(\"Shape of X_validation: \", X_validation.shape)\n",
        "print(\"Shape of y_validation: \", y_validation.shape)\n",
        "print(\"Shape of X_test: \", X_test.shape)\n",
        "print(\"Shape of y_test: \", y_test.shape)"
      ],
      "metadata": {
        "id": "m_4Y0oVZOVUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pandas DataFrames from the datasets\n",
        "train_index = range(0,len(X_train))\n",
        "validation_index = range(len(X_train), \\\n",
        "                         len(X_train)+len(X_validation))\n",
        "test_index = range(len(X_train)+len(X_validation), \\\n",
        "                   len(X_train)+len(X_validation)+len(X_test))\n",
        "\n",
        "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
        "y_train = pd.Series(data=y_train,index=train_index)\n",
        "\n",
        "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
        "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
        "\n",
        "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
        "y_test = pd.Series(data=y_test,index=test_index)"
      ],
      "metadata": {
        "id": "L62FWqSvOqJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()"
      ],
      "metadata": {
        "id": "jsCnxiuxPuvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "v0RDxprrQYEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def view_digit(example):\n",
        "    label = y_train.loc[example]\n",
        "    image = X_train.loc[example,:].values.reshape([28,28])\n",
        "    plt.title('Example: %d  Label: %d' % (example, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2zPFg0U3QdTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_digit(0)"
      ],
      "metadata": {
        "id": "VkmfrP9OQg8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(series):\n",
        "    label_binarizer = pp.LabelBinarizer()\n",
        "    label_binarizer.fit(range(max(series)+1))\n",
        "    return label_binarizer.transform(series)"
      ],
      "metadata": {
        "id": "zPEYmYGEQiKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_one_hot(originalSeries, newSeries):\n",
        "    label_binarizer = pp.LabelBinarizer()\n",
        "    label_binarizer.fit(range(max(originalSeries)+1))\n",
        "    return label_binarizer.inverse_transform(newSeries)"
      ],
      "metadata": {
        "id": "pnSoekGfRPFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_oneHot = one_hot(y_train)\n",
        "y_validation_oneHot = one_hot(y_validation)\n",
        "y_test_oneHot = one_hot(y_test)"
      ],
      "metadata": {
        "id": "38WiJlCERY2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_oneHot[0]"
      ],
      "metadata": {
        "id": "MesSZIvmRfMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Dimensionality Reduction\n",
        "<h2> PCA"
      ],
      "metadata": {
        "id": "AwogB33lRkjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = 784\n",
        "whiten = False\n",
        "random_state = 2018\n",
        "\n",
        "pca = PCA(n_components=n_components, whiten=whiten, random_state=random_state)\n",
        "\n",
        "X_train_PCA = pca.fit_transform(X_train)\n",
        "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)"
      ],
      "metadata": {
        "id": "VA2hUMh4Rnow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of Variance Captured by 784 principal components\n",
        "print(\"Variance Explained by all 784 principal components: \", \\\n",
        "      sum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "SDETnUpISpYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of Variance Captured by X principal components\n",
        "importanceOfPrincipalComponents = \\\n",
        "    pd.DataFrame(data=pca.explained_variance_ratio_)\n",
        "importanceOfPrincipalComponents = importanceOfPrincipalComponents.T\n",
        "\n",
        "print('Variance Captured by First 10 Principal Components: ',\n",
        "      importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\n",
        "print('Variance Captured by First 20 Principal Components: ',\n",
        "      importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\n",
        "print('Variance Captured by First 50 Principal Components: ',\n",
        "      importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\n",
        "print('Variance Captured by First 100 Principal Components: ',\n",
        "      importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\n",
        "print('Variance Captured by First 200 Principal Components: ',\n",
        "      importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\n",
        "print('Variance Captured by First 300 Principal Components: ',\n",
        "      importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)"
      ],
      "metadata": {
        "id": "3aK3vyyLS01R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View percentage captured by first X components\n",
        "sns.set(rc={'figure.figsize':(10,10)})\n",
        "sns.barplot(data=importanceOfPrincipalComponents.loc[:,0:9],color='k')"
      ],
      "metadata": {
        "id": "DkCO30zYTC2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define scatterplot function \n",
        "def scatterPlot(xDF, yDF, algoName):\n",
        "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
        "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
        "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
        "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
        "               data=tempDF, fit_reg=False)\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(\"Separation of Observations using \"+algoName)"
      ],
      "metadata": {
        "id": "eU2P8NI0VDFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scatterPlot(X_train_PCA, y_train, \"PCA\")"
      ],
      "metadata": {
        "id": "ZLuI0atpVK6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View two random dimensions\n",
        "X_train_scatter = pd.DataFrame(data=X_train.loc[:,[350,406]], index=X_train.index)\n",
        "X_train_scatter = pd.concat((X_train_scatter,y_train), axis=1, join=\"inner\")\n",
        "X_train_scatter.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
        "sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", data=X_train_scatter, fit_reg=False)\n",
        "ax = plt.gca()\n",
        "ax.set_title(\"Separation of Observations Using Original Feature Set\")"
      ],
      "metadata": {
        "id": "NLlTuK1iVnI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Incremental PCA"
      ],
      "metadata": {
        "id": "AetLp09KWQs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_components = 784\n",
        "batch_size = None\n",
        "\n",
        "incrementalPCA = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
        "\n",
        "X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)\n",
        "X_train_incrementalPCA = pd.DataFrame(data=X_train_incrementalPCA, index=train_index)\n",
        "\n",
        "X_validation_incrementalPCA = incrementalPCA.transform(X_validation)\n",
        "X_validation_incrementalPCA = pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_incrementalPCA, y_train, \"Incremental PCA\")"
      ],
      "metadata": {
        "id": "YbfyM40eWSRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse PCA"
      ],
      "metadata": {
        "id": "bCOPB0_TXPAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse PCA\n",
        "from sklearn.decomposition import SparsePCA\n",
        "\n",
        "n_components = 100\n",
        "alpha = 0.0001\n",
        "random_state = 2018\n",
        "n_jobs = -1\n",
        "\n",
        "sparsePCA = SparsePCA(n_components=n_components, \\\n",
        "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
        "\n",
        "sparsePCA.fit(X_train.loc[:10000,:])\n",
        "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
        "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\n",
        "\n",
        "X_validation_sparsePCA = sparsePCA.transform(X_validation)\n",
        "X_validation_sparsePCA = \\\n",
        "    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")"
      ],
      "metadata": {
        "id": "KhVWPxWtXQlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel PCA"
      ],
      "metadata": {
        "id": "aWqvPW2DXSty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "n_components = 100\n",
        "kernel = 'rbf'\n",
        "gamma = None\n",
        "random_state = 2018\n",
        "n_jobs = 1\n",
        "\n",
        "kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n",
        "                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)\n",
        "\n",
        "kernelPCA.fit(X_train.loc[:10000,:])\n",
        "X_train_kernelPCA = kernelPCA.transform(X_train)\n",
        "X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\n",
        "\n",
        "X_validation_kernelPCA = kernelPCA.transform(X_validation)\n",
        "X_validation_kernelPCA = \\\n",
        "    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")\n"
      ],
      "metadata": {
        "id": "tTTzEc6gXTfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Singular Value Decomposition"
      ],
      "metadata": {
        "id": "tAPpFkkbXVB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Singular Value Decomposition\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "n_components = 200\n",
        "algorithm = 'randomized'\n",
        "n_iter = 5\n",
        "random_state = 2018\n",
        "\n",
        "svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\n",
        "                   n_iter=n_iter, random_state=random_state)\n",
        "\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\n",
        "\n",
        "X_validation_svd = svd.transform(X_validation)\n",
        "X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")"
      ],
      "metadata": {
        "id": "2SXfeuG8XX4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Random Projection"
      ],
      "metadata": {
        "id": "S17r1ShCXaMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Random Projection\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "n_components = 'auto'\n",
        "eps = 0.5\n",
        "random_state = 2018\n",
        "\n",
        "GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \\\n",
        "                               random_state=random_state)\n",
        "\n",
        "X_train_GRP = GRP.fit_transform(X_train)\n",
        "X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\n",
        "\n",
        "X_validation_GRP = GRP.transform(X_validation)\n",
        "X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")"
      ],
      "metadata": {
        "id": "_SYP-NNUXcTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Random Projection"
      ],
      "metadata": {
        "id": "e5XegdzLXe0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse Random Projection\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "\n",
        "n_components = 'auto'\n",
        "density = 'auto'\n",
        "eps = 0.5\n",
        "dense_output = False\n",
        "random_state = 2018\n",
        "\n",
        "SRP = SparseRandomProjection(n_components=n_components, \\\n",
        "        density=density, eps=eps, dense_output=dense_output, \\\n",
        "        random_state=random_state)\n",
        "\n",
        "X_train_SRP = SRP.fit_transform(X_train)\n",
        "X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\n",
        "\n",
        "X_validation_SRP = SRP.transform(X_validation)\n",
        "X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")"
      ],
      "metadata": {
        "id": "yRk9uqJxXgL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-Linear Dimensionality Reduction\n",
        "<h2> Isomap"
      ],
      "metadata": {
        "id": "1HVUj0JoXjkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import Isomap\n",
        "\n",
        "n_neighbors = 5\n",
        "n_components = 10\n",
        "n_jobs = 4\n",
        "\n",
        "isomap = Isomap(n_neighbors=n_neighbors, \\\n",
        "                n_components=n_components, n_jobs=n_jobs)\n",
        "\n",
        "isomap.fit(X_train.loc[0:5000,:])\n",
        "X_train_isomap = isomap.transform(X_train)\n",
        "X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\n",
        "\n",
        "X_validation_isomap = isomap.transform(X_validation)\n",
        "X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\n",
        "                                   index=validation_index)\n",
        "\n",
        "scatterPlot(X_train_isomap, y_train, \"Isomap\")"
      ],
      "metadata": {
        "id": "pwCpyGCGXmTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DAExUJynXpPa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}